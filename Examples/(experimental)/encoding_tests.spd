

// OK... so I basically don't get this
// it is saying... that to compress SC_Function.spd...
// just the pure offsets would require 26KB. how is that possible?
// mz compresses it down to 17KB? Thats with 16-bit offsets.
// and mz only uses 4K offsets mostly! Am I overprocessing this?

// reconstitution says that its actually the same! Definitely not pumping in extra data.
// i think I should just give up.

// still its a good test of speedie! Its working very well!


main (|file.existing| file)
	|| orig = file.ReadAll			#require
	|Compressor| C
	C.init
	|| back = C.Process(orig)
	if back != orig
		"Couldn't decompress! ${orig.strsize} --> ${back.strsize}"
		



struct RangeCollector (FastBuff)
	|int|			Count
	|uint16[32]|	Histo
	
	
	function Trip
		if .count
			self <~ .count
			.count = 0
	
	syntax append (|int| v)
		|| bin = 0
		if v
			bin = v.Log2
		.Histo[bin]++
		while v >= 64K // what?
			super <~ (64K-1)|uint16|
			v -= 64K
		super <~ v|uint16|
	
	
	function Display (|string| name)
		|| end = .curr|&uint16|
		|| cur = .start|&uint16|
		|| fs = faststring()
		fs <~ "
$name (${end - cur}):
"
		for i in 32
			|| h = .histo[i]
				fs <~ "    ${i+1} = $h\n"
		if app.yes("x") or app.yes("extra")
			fs <~ "\n"
			while
				fs <~ (*cur++)|int|
				if cur >= end
					exit
				fs <~ ", "
		printline fs

	prototype Judger (|&RangeCollector| self, |int|)
			


struct Compressor
	|int| 			 OrigLen
	|FastBuff|		 Root
	|FastBuff|       String
	|RangeCollector| Offsets
	|RangeCollector| Lengths
	|RangeCollector| RangeOff
	|RangeCollector| RangeStr

	function Init
		.root.Alloc(1MB)
		.root.divide(.String,     256KB)
		.root.divide(.Offsets,    256KB)
		.root.divide(.Lengths,    256KB)
		.root.divide(.RangeOff,   128KB)
		.root.divide(.RangeStr,   128KB)
	

	function AddOffset (|ivec2| P)
		.RangeStr.trip
		.rangeoff.count++
		.Offsets <~ p.x
		.lengths <~ p.y-2

	function AddByte (|byte| B)
		.RangeOff.trip
		.rangestr.count++
		.string <~ b
		
	function Process (|string| s, |string|)
		.OrigLen = s.Length
		.init
		.ProcessSub(s)
		.rangeoff.trip
		.RangeStr.trip
		.display
		.Judge
		return .Reconstitute
		
	function Reconstitute (|string|)
		|| n = .RangeOff.Length
		|| fs = FastString()
		|| io = 0
		.string.Reset

		for i in n
			|| rs = (.RangeStr.start|&uint16|)[i]|int|
			for rs
				fs <~ .String.Byte
			|| ro = (.RangeOff.start|&uint16|)[i]|int|
			for ro
				|| b = (.Offsets.start|&uint16|)[io]|int|
				|| l = (.lengths.start|&uint16|)[io]|int| + 2
				ifn (Fs.length+l <= .OrigLen)
					error ("too much data")
					exit
				fs.appendmemory(fs.ResultPtr + fs.length - b, l)
				io++
		
		return fs


	module
		function J16 (RangeCollector.Judger)
			return .Length*2
		function J8 (RangeCollector.Judger)
			return .Length

	
	function JudgeIt (|RangeCollector.Judger| J, |&rangecollector| x, |string| name,  |int|)
		rz = (J)(x)
		"$name: $rz (${rz.strsize})"
	
	
	function Judge 
		|| x = .String.Length
		"Raw:   $x (${x.strsize})"
		x += .JudgeIt(J16, .offsets,  "offsets")
		x += .JudgeIt(j8,  .lengths,  "lengths")
		x += .JudgeIt(j8,  .rangeoff, "off++")
		x += .JudgeIt(j8,  .rangestr, "str++")
		"Orig:  $.OrigLen (${.OrigLen.strsize})"
		"Total: $x (${x.strsize})"

	
	function Display
		printline "Strings:"
		printline .string.TmpStr
		.Offsets.display("offsets")
		.Lengths.display("lengths")
		.RangeOff.display("off++")
		.RangeStr.display("str++")
		
	
	function ProcessSub (|string| s)
		|| z = s.addr
		|| a = z
		|| b = a + s.length
		while a < b
			|| P = findlongest(z, a, b)
				.AddOffset(P)
				a += p.y
			  else
				.AddByte(*a++)


	module
		function Match (|&byte| curr, |&byte| from, |&byte| to, |int|)
			while (from < to) and (*curr++ == *from++)
				rz++
			
		function FindLongest (|&byte| base, |&byte| from, |&byte| to, |ivec2|)
			|| curr = base
			while curr < from
				|| n = .match(curr, from, to)
				if n > 1 and n > rz.y
					rz.x = from - curr
					rz.y = n
				curr++
			

		

/*
OK... so how to make this work? with the numeric compression? so that smaller numbers get less bits?

I think... 12 bits is a nice thing... on the whole. Honestly. We can use 256 for single bytes.

Keep 2048 for longer offsets... so we get about 1800 shorter offsets. We can store length-bits elsewhere.

We'd need to output a histo-gram... sigh. More work. So... then what? how to store longer offsets? Lets assume we add 6 more bits. So... now we have 18 bits. But actually only 17 really. So we can get offsets of 128K. I think long offsets work very well. Can we noob this further? Why not?

I wonder... how does "Chained noobing" work? Lets say we take 1/4 of any space. And lets assume bytes.

So... normally have 256. But we noob it down to 192. That gives us 6 bits to use on the next! So... That is 14-bits. So lets make a table

1 byte:  1-192         -->  6  bits offered
2 byte:  193-12480     -->  12 bits offered
3 byte:  12481-786432  -->  18 bits offered


...



We could even rolz this? Just store an array of 256-offsets. In fact... we could do 2 bytes also, just use 64k offsets. Although it would be nice to have both... or even neither. So that would be like 2 more bits used. sigh. Maybe not then? Unless it were not optional. Actually no... we need a table of ALL the previous offsets of that byte.

*/