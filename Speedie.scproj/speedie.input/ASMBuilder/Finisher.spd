

// MiniOptimisations, miniopts, optimize
function Assembler.FinishMost
	|| A = .Curr
	|| S = .FuncStart
	|| Length = 0
	
	target debug
		|ASM[4]| TestSpace
	
	while A > S
		if s isa asm.knst
			S.ConstFinish
		  elseif S.JumpPrm
			s.JumpImprove(self)

		|| Predicted = s.guesssize
		length += Predicted
		
		target debug
			|| Start = &TestSpace[0]
			|| After = s.µRenderInto(Start, start+4)
			|| Actual = After - Start
			if Actual != Predicted // uugh		
		S++
		
	.out.µASMLength = Length


function fatasm.Nop_Encoder (Asm_encoder)
	return curr


function fatasm.Halt_Encoder (Asm_encoder)
	curr++[] = .prms[0]&0xFFFfff // a shame to waste this opcode.
	curr++[] = 0				 // we'd have to move the 2/3 codes out of 0. Or up by 1.
	return curr


function fatasm.KNST_Encoder (Asm_encoder)
	rz = ASM_ConstStretchy.Encode(self, curr, after)
	|| op = .op
	if op != asm.knst
		rz++[] = .prms[4]
		if op == asm.knst3
			rz++[] = .prms[5]


function fatasm.SimpleConst (|uint64| v, |int| Space,  |bool|)
	|| negate = v|int64| < 0
		v = ~v
	|| sh = 64-Space
	if (v << sh) >> sh == v
		.prms[2] = negate|int|
		.prms[3] = v & 17~bits
		.prms[4] = v >> 17
		.prms[5] = v >> 49
		return true
//		Cond	1		// 0 == always, 1 = only if dest == 0
//		Inv		1
//		Value	17


function fatasm.ConstFinish 
	|| v = .const|uint64|
	if .SimpleConst(v, 17)
		._op = asm.KNST // already set, but its for clarity
		return
	if .RotateConst(V)
		._op = asm.KNSR
		return
	if .SimpleConst(v, 49)
		._op = asm.KNST2
		return
	.SimpleConst(v, 64)
	._op = asm.KNST3


function uint64.rotl1 (|uint64|)
	return (self << 1) ||| (self >> 63)


function fatasm.RotateConst (|uint64| v, |bool|)
	|| BC = V.CountBits
	|| inv = 64-BC <= 12
		V = ~V
	  else
		require BC <= 12

	for i in 64
		if v <= (12~bits)|uint64|
			.p1 = i
			.p2 = inv|int|
			.p3 = v|uint|
			return true
		v = v.rotl1


