

// vm.box
forms
	U0:
	U1:		r1 R
	U2:		r1 R,  r2 r
	U3:		r1 R,  r2 r,  r3 r
	U4:		r1 R,  r2 r,  r3 r,  r4 r


	Func
		r1		R
		JUMP	l
		Prm1	x
		Prm2	x

// func needs the reg/return wierdness defined also, but as an "ignored" value
// so it comes after the prm info. Like 3 prms takes 15 bits, then 3 more bits for wierdness
// using 32-bits, we can fit 5 regs leaving 7 bits for the total info with 1 bit left to spare.
	ForeignFunc
		r1		R
		Table	l			// functions in registers use the first 32 values...
		Prm1	x
		Prm2	x
		
	Tail
		JUMP	l
		Prm1	x
		
	FuncAddr
		r1		R
		Library 1
		Index	l

	Alloc
		r1		R
		Amount	l
	
	Jump
		JUMP	j
		
	RotateConst
		r1		R
		Rot		6
		Inv		1
		Value	12

	JCmpEq
		r1		r
		r2		r
		LSmall	1
		RSmall	1
		Jmp		j

	HALT
		r1		r
		r2		r
		r3		r
		r4		r
		IsOK	4

	AddOrSubM
		r1		R
		r2		r
		r3		r
		Sh		6

	JCmpF
		r1		r
		r2		r
		Cmp		3
		Jmp		j

	TERN
		r1		R
		r2		r
		r3		r
		r4		r
		Small	1
		
	JCmpI
		r1		r
		r2		r
		Cmp		4
		Jmp		j

	CmpI
		r1		R
		r2		r
		r3		r
		Cmp		4

	CmpF
		r1		R
		r2		r
		r3		r
		Cmp		4

	Compare4
		r1		R
		r2		r
		r3		r
		r4		r
		Small	1
		
		
	JCmpK // assume signed?
		r1		r
		K		9
		Jmp		j

	Bra		// bra:
		r1		r
		Small	1
		Jmp		j
	
	Loop
		r1		R // assume one is an int64?
		r2		r
		Small	1
		Jmp		j
	
	GTable
		r1		R
		Mode	1
		Add		l

	
	GObj
		r1		R
		Mode	1
		Ref		1
		Add		l
	
	Read
		r1		R	// dest (big r!)
		r2		r	// Base
		r3		r	// VarAdd
		Offset	7	// Const A
		move	2

	Write
		r1		r	// Src (small r!)
		r2		r	// Base
		r3		r	// VarAdd
		Offset	7	// Const A
		move	2

	MemUtil
		r1		R
		r2		r
		Op		3
		N		l
	

	CNTC
		r1		R
		r2		r
		offset	6
		cnst	6
		size	2 // 1, 2, 4, 8
		// *p1[offset] += cnst

	BFLD
		r1		R
		r2		r
		up		6
		down	6
		sign	1

	BFLS
		r1		P
		r2		r
		up		6
		down	6
		sign	1

	Convert
		r1		R
		r2		r
		Mode	4
		
	RET
		r1		r
		Value	19

	REQ
		r1		r		// 
		r2		r		// 
		Mode	4		// compare mode for u1 & u2 (==, !=, >, >=, AND, |, & )
						// AND can ! either left or right side
		Val		10		// bitor r2 with Val. 
	
	Div
		r1		R
		r2		R
		r3		r
		r4		r
		Kind	2
	
	Trap
		r1		r
		Upon	l

	Float
		r1		R
		r2		r
		r3		r
		r4		r
		D		1

	FloatConst
		r1		R
		r2		r
		High	14

	ConstStretchy // rename to just Const
		r1		R
		Cond	1		// 0 == always, 1 = only if dest == 0
		Inv		1
		Value	17
	
	MemoryCopy
		r1		r		// dest
		r2		r		// src
		Length  14
//		Index	8
//		Length	6       // do this later... too fiddly right now
						// its nicer on a low-level, I guess?
		
	AddB
		r1		R
		K		l
	
	AddK
		r1		R
		r2		r
		K		l
		
	AddAK
		r1		R
		r2		R
		K		l
	
	Div2
		r1		R
		r2		r
		Sh		6   // 6 bits left unused? Oh well?
		Clear	3
	
	Shift
		r1		R
		r2		r
		r3		r
		Sh		9
	
	Swap
		r1		R
		r2		R
		r3		R
		r4		R

	RefSet1
		r1		R
		r2		r
	
	RefReturn
		r1		r
		r2		r
		r3		r
		r4		r

	RefSetApart
		r1		r
		r2		r
		r3		r
	
	RefSet2
		r1		r		// Small R
		r2		r
		Decr	1
		Offset	l
	
	RefSet3
		r1		R		// Big R
		r2		r
		Decr	1
		Offset	l
	
	RefDecrMem
		r1		r
		Count	7
		Offset	l

	VecMix
		r1		R
		r2		r // can load a float, vec2, or vec3
		r3		r
		r4		r
		Mode	3 // only 6 actual cases. Only list possible entries.


	VecGet
		r1		R
		r2		r
		r3		r
		Ind		2

	VecSet
		r1		P
		r2		r
		r3		r
		Ind		2
	
	
	VecBuild
		r1		R
		r2		r
		r3		r
		r4		r
		r5		r

	VecSwizzle
		r1		R
		r2		r
		Fields	12
		
	VecConst
		r1		R
		K1		l
		K2		x
		
	VecInc
		r1		R
		r2		P
		part	2
		Amount	l
		
	
// this file:
// 1) builds the c++ codes, and the jump-table, and the ASM accessor functions
// 2) builds the instruction-info speedie uses exec can just include this file too.
// 3) builds the Âµforms



#!						 (((((STOP)))))
/**/ EROR (HALT)
		if (u2 == u1 and u3 == u4)
			VMFinish

/**/ TRAP (Trap)
		// actually a noop. the debugger itself will put this in the shadow
		0


#!						 (((((UTIL)))))

/**/ NOOP (U0)	// does nothing // (could be replaced with bitor 0?)
		i1 = i1
		#!NOOP
/**/ DUMB (U2)
		0 // Can be added or removed... just to align the jumps really!
/**/ TIME (U2)
		Time_(r, Op)
/**/ CONV (Convert)
		RegConv(r, Op)
					// for SPD~>ARM conversion
					// we can just do:	double from double --> int64reg = F64reg 
					//					int64  from int64  --> F64Reg = int64
					// floats or other size ints shouldn't be affected.

/**/ FEET (AddK)	// Request available features. do nothing for now
		u1 = 0		// allows doing stuff like initing hardware.



#!						 (((((FUNC)))))
/**/ GRAB (U2) 
		u1 = (uint64)(&u2)

/**/ RET (Ret)				// ret: 
		Code = ReturnFromFunc(vm, r, Op)

POS 2
	TAIL (Tail)
		TailStack(vm, r, Code, Op)


POS 24 // force position
	FNC (Func)			// func: 
		Code = BumpStack(vm, r, Code+1, Op, *Code)

POS 25
	FNC3 (Func)
		Code = BumpStack(vm, r, Code+2, Op, Code64)

POS 26
	FNCX (ForeignFunc)
		ForeignFunc(vm, Code, r, Op, *Code)
		Code++ 

POS 27
	FNCX3 (ForeignFunc)
		ForeignFunc(vm, Code, r, Op, Code64) 
		Code += 2

// we could make the function call a relative address
// like add a number to the current PC?
// It doesn't work for libglobs, though. For now... just do prototypes
// as globals.

/**/ AFNC (FuncAddr)
		i1 = FuncAddr(vm, Op)


/**/ ALLO (Alloc)
		AllocStack(vm, r, Op)



#!						 (((((Consts)))))
/**/ KNSR (RotateConst)
		RotateConst(r, Op)

/**/ KNST (ConstStretchy)
		LoadConst(r, Op, 0)

POS 4
	KNST2 (ConstStretchy)
		LoadConst(r, Op, *Code++)
POS 5
	KNST3 (ConstStretchy)			// this leaves 17 bits unused. Perhaps can be reused for SIMD?
		LoadConst(r, Op, *((uint64*)(Code)))
		Code+=2



#!						 (((((Math)))))

/**/ ADDB (AddB) // add big
		i1 = i1 + (U1_Li<<12)
/**/ ADDK (AddK)
		i1 = i2 + U2_Li
/**/ ADAK (AddAK) // after-increment (post-increment)
		i1 = i2
		i2 = i2 + U2_Li
/**/ MULK  (AddK)
		i1 = i2 * U2_Li
/**/ ADD  (Shift)
		i1 = i2 + (i3 << Shift_Shu)
/**/ SUB  (Shift)              						// must come after ADD
		i1 = (i2 - i3) >> Shift_Shu
/**/ MUL  (U4)
		i1 = (i2 * i3) + i4
/**/ DIV (Div)
		DivMath(r, Op)
/**/ DIV2 (Div2)
		if (i2 < 0)	// quick divide by power of 2 on a signed int
			i1 = (((i2+((1<<Div2_Shu)-1)) >> Div2_Shu) << Div2_Clearu) >> Div2_Clearu
		else
			i1 = ((i2 >> Div2_Shu) << Div2_Clearu) >> Div2_Clearu

/**/ ICLM (Compare4)
		i1 = std_clamp(btc(i2), btc(i3), btc(i4))
/**/ UCLM (Compare4)
		u1 = std_clamp(btc(u2), btc(u3), btc(u4))


#!					((((((((32-bit math))))))))
// Need signed/unsigned versions?        0xFFFFffff != -1|int64|
/**/ ADDM (AddOrSubM) // mixed math
		i1 = i2 + (ii3 << AddOrSubM_Shu)
/**/ SUBM (AddOrSubM)
		i1 = (i2 - ii3) >> AddOrSubM_Shu
/**/ DIVS (Div)
		DivMath32(r, Op)
/**/ MULS  (U4)
		i1 = (ii2 * ii3) + ii4



#!						 (((((Bitshifts)))))
/**/ BFLG  (BFLD)
		if (BFLD_signu)
			i1 = ((i2 << BFLD_upu) >> BFLD_downu)
		else
			u1 = ((u2 << BFLD_upu) >> BFLD_downu)	
/**/ BFLS  (Shift)
		u1 = u2 ||| (u3 << Shift_Shu)
/**/ BRSS  (Shift)
		i1 = ((i2 << Shift_Shu) >> Shift_Shu) >> i3
/**/ BRSH  (Shift)
		u1 = ((u2 << Shift_Shu) >> Shift_Shu) >> u3
/**/ BLSH  (Shift) 
		u1 = ((u2 << u3) << Shift_Shu) >> Shift_Shu



#!						 (((((BitOps)))))
/**/ BAND  (Shift)
		u1 = u2  &  ((u3 << Shift_Shu) >> Shift_Shu)
/**/ BORR  (Shift)
		u1 = u2 ||| ((u3 << Shift_Shu) >> Shift_Shu)
/**/ BXOR  (Shift)
		u1 = u2  ^  ((u3 << Shift_Shu) >> Shift_Shu)
/**/ BNOT  (Shift)
		u1 = ~u2 &  ((~u3 << Shift_Shu) >> Shift_Shu)
/**/ BANK  (AddK)
		u1 = u2  &  U2_Li
/**/ BORK  (AddK)
		u1 = u2 ||| U2_Li
/**/ BXRK  (AddK)
		u1 = u2  ^  U2_Li



#!						 (((((MiniCompare)))))
/**/ EQUL  (CmpI)
		u1 = BitComp(r, Op)

/**/ TERN  (TERN)
		if (u2) // needs a shift
			r[n1] = r[n3]
		else
			r[n1] = r[n4]

/**/ CMPI  (CmpI)
		CompI(r, Op)

/**/ CMPF  (CmpF)
		CompF(r, Op)



#!						 (((((Branches)))))
/**/ JUMP (jump)
		Code += l0

/**/ JMPI  (JCmpI)
		Code = JumpI(r, Op, Code)

/**/ JMPF  (JCmpF)
		Code = JumpF(r, Op, Code)

/**/ JMPE  (JCmpEq)
		Code = JumpEq(r, Op, Code)

/**/ JMPN  (JCmpEq)
		Code = JumpNeq(r, Op, Code)

/**/ JMKM  (JCmpK)						// (jump more)
		Code = JumpK(r, Op, Code)

/**/ JMKL  (JCmpK)						// (jump less)
		Code = JumpKN(r, Op, Code)

/**/ JMKE  (JCmpK)
		Code = JumpKE(r, Op, Code)

/**/ JMKN  (JCmpK) 
		Code = JumpKNE(r, Op, Code)
	
/**/ JBOR  (Bra)
		if (u1<<(Bra_Smallu<<5))
			Code += Bra_Jmpi

/**/ JBAN  (Bra)				
		if (!(u1<<(Bra_Smallu<<5)))
			Code += Bra_Jmpi

/**/ LUPU  (Loop)
		if (ii1++ < ii2)
			Code += Loop_Jmpi

/**/ LUPD  (Loop)
		if (ii1-- > ii2)
			Code += Loop_Jmpi



#!						 (((((Refs)))))
/**/ RFUN  (RefSetApart)
		SetRefApart(r, Op) // freeifdead, safedecr, incr

/**/ RFST  (RefSet1)
		SetRefBasic(r, Op)		

/**/ RFWR  (RefSet2)							// RefCount reg --> mem
		SetRefRegToMem(r, Op)

/**/ RFRD  (RefSet3)							// RefCount reg <-- mem
		SetRefMemToReg(r, Op)

/**/ RALO  (U2)	// could be nicer to use a gtab const directly.
		o1 = alloc(o2) // 1 less instruction... gtabs suck anyway.
	
/**/ RFRT  (RefReturn)							// this should be a return...
		Code = DeRefRegs(vm, r, Op)				// its ONLY used for returns...



#!						(((((Memory)))))
/**/ GOBJ  (GObj)
		o1 = strs(vm, Op)
/**/ GTAB  (gtable)
		u1 = table(vm, Op)
/**/ RD1U  (Read)
		u1 = mem1(uint8)
		mem2(uint8)
/**/ RD1S  (Read)
		u1 = mem1(char)
		mem2(char)
/**/ RD2U  (Read)
		u1 = mem1(u16)
		mem2(u16)
/**/ RD2S  (Read)
		u1 = mem1(s16)
		mem2(s16)
/**/ RD4U  (Read)
		u1 = mem1(u32)
		mem2(u32)
/**/ RD4S  (Read)
		u1 = mem0(int)
		mem2(int)
/**/ RD8U  (Read)
		u1 = mem1(u64)
		mem2(u64)
/**/ RD16  (Read)
		((ivec4*)r)[n1] = mem1(ivec4)
		mem2(ivec4)
/**/ WR1U  (Write)
		mem1(uint8) = u1
		mem2(uint8)
/**/ WR2U  (Write)
		mem1(u16) = u1
		mem2(u16)
/**/ WR4U  (Write)
		#! xcode only complains about this one? ?
		mem1(u32) = (u32)u1
		mem2(u32)
/**/ WR8U  (Write)
		mem1(u64) = u1
		mem2(u64)
/**/ WR16  (Write)
		mem1(ivec4) = (ivec4)(((ivec4*)r)[n1])
		mem2(ivec4)

/**/ CNTC  (CNTC)
		IncrementAddr(r, Op, 1) // same with a few others actually

/**/ CNTD  (CNTC)
		IncrementAddr(r, Op, 0)

// rename to WRCP // write copy
/**/ WCPY  (MemoryCopy) // can zero memory, if we pass r0 as src.
		MemCopyRDWR(r, Op)



#!						(((((Float)))))

/**/ FADD  (Float)
		if (!Float_Du)
			d1 = d2 + d3 - d4
		else
			f1 = f2 + f3 - f4

/**/ FADK (FloatConst)
		// we just want simple things like MyFloat += 2.0
		f1 += f2 + FloatIncr1(Op)

/**/ FMUL  (Float)
		if (!Float_Du)
			d1 = (d2 * d3)+d4
		else
			f1 = (f2 * f3)+f4

/**/ FMLK  (FloatConst)
		f1 = f2 * FloatIncr1(Op)

/**/ FDIV  (float)
		if (!Float_Du)
			d1 = d2 / d3
		else
			f1 = f2 / f3

/**/ FMOD  (float)
		if (!Float_Du)
			d1 = fmod(d2, d3)
		else
			f1 = fmodf(f2, f3)

/**/ FMAX (Float)
		if (!Float_Du)
			d1 = std_max(d2,d3)
		else
			f1 = std_max(f2,f3)

/**/ FMIN (Float)
		if (!Float_Du)
			d1 = std_min(d2,d3)
		else
			f1 = std_min(f2,f3)

/**/ FCLM (Float)
		if (!Float_Du)
			d1 = std_clamp(d2,d3,d4)
		else
			f1 = std_clamp(f2,f3,f4)



#!						(((((Vectors)))))
/**/ VGET (VecGet)
		i1 = iv2[n3+u4]
/**/ VSET (VecSet)
		iv1[n3+u4] = i2
/**/ VBLD (VecBuild)
		v1 = VBuild(r, Op)
/**/ VSWZ (VecSwizzle)
		v1 = VSwiz(r, Op)
/**/ VMOV (U4)
		iv1 = iv2
		iv3 = iv4



#!						(((((Float VecMath)))))
/**/ VADD  (Float)
		v1 = v2 + v3 - v4
/**/ VADK (FloatConst)
		v1 += v2 + FloatIncr1(Op)
/**/ VMUL  (Float)
		v1 = (v2 * v3)+v4
/**/ VMLK  (FloatConst)
		v1 = v2 * FloatIncr1(Op)
/**/ VDIV  (float)
		v1 = v2 / v3
/**/ VMOD  (float)
		v1 = JB_vec4_Mod(v2, v3)
/**/ VMAX (U3)
		v1 = JB_vec4_Max(v2,v3)
/**/ VMIN (U3)
		v1 = JB_vec4_Min(v2,v3)



#!						(((((Integer VecMath)))))
/**/ QADD  (Shift)
		iv1 = iv2 + (iv3 << Shift_Shu)
/**/ QSUB  (Shift)              						// must come after ADD
		iv1 = (iv2 - iv3) >> Shift_Shu
/**/ QADK (AddK)
		iv1 = iv2 + U2_Li
/**/ QMUL  (U4)
		iv1 = (iv2 * iv3) + iv4
/**/ QDIV (Div)
		iv1 = iv2 / iv3
/**/ QCLM (U4)
		iv1 = JB_ivec4_ClampVec(iv2,iv3,iv4)
/**/ QINC  (VecInc)
		i1 = iv2[VecInc_partu] + VecInc_Amounti
		iv2[VecInc_partu] = i1



#!						 (((((Vector Bitshifts)))))
/**/ QFLG  (BFLD)
		iv1 = ((iv2 << BFLD_upu) >> BFLD_downu)
/**/ QFLS  (BFLS)
		iv1 |= ((iv2 << BFLD_upu) >> BFLD_downu)
/**/ QRSS  (Shift)
		iv1 = (iv2 >> Shift_Shu) >> iv3
/**/ QRSH  (Shift)
		uv1 = (uv2 >> Shift_Shu) >> uv3
/**/ QLSH  (Shift) 
		uv1 = (uv2 << uv3) << Shift_Shu



#!						 (((((Vector BitOps)))))
/**/ QAND  (Shift)
		uv1 = uv2  &  (uv3 ||| Shift_Shu)
/**/ QXOR  (Shift)
		uv1 = uv2  ^  (uv3 ||| Shift_Shu)
/**/ QORR  (Shift)
		uv1 = uv2 ||| (uv3 ||| Shift_Shu)
/**/ QNOT  (Shift)
		uv1 = ~uv2 & ~(uv3 ||| Shift_Shu)
/**/ QCNV (Convert)
		VecConv(r, Op)




#!						(((((Intrinsics!)))))
/**/ BSTT  (addk)
		u1 = bitstats(u2, U2_Li)
  
