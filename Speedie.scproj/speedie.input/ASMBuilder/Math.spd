

prototype fn_OpASM          (|&Assembler| self, |asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |asmreg|)
prototype fn_ASMConstifier  (|asmreg| L, |asmreg| R, |int64|)


extend asmreg
	function BoolAnswer (|asmreg|)
		.µtype = DataTypeCode.bool
		
		is AlreadyNegated
		isnt negate
		if self is CondRequest
			isnt condrequest
			is CondAnswer
		return self 


extend Assembler
	function EqualsInt (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |asmreg|)
		|| negate = dest is negate
		|| Res = dest.BoolAnswer						// Hmmm... so what should it be?
		if  L is ConstOutput  and  R is ConstOutput
			.Nop2Consts(r, l)
			|| num = ((l.const==r.const)!=negate)|int|
			return .NumToReg(exp, num, res, DataTypeCode.bool)
		
		if dest isnt CondRequest
			dest = .TempTyped(exp, dest, TypeBool!.TypeNormal)
			|| Mode = negate|int| xor 1
			mode |= l.Small << 2
			mode |= R.Small << 3
			return exp.EQUL(dest, L, R, mode) * dest
		
		
		|| RS = r.small
		|&fatasm| fat
		if l.reg
			|| LS = L.Small
			if negate
				fat = exp.JMPN(l, r, LS, RS, 0)
			  else
				fat = exp.JMPE(l, r, LS, RS, 0)
		  else
			if negate
				fat = exp.JBAN(r, RS, 0)
			  else
				fat = exp.JBOR(r, RS, 0)
		return fat * res

	
	function Exists (|ASMReg| dest, |ASMReg| L, |message| exp, |ASMReg|)
		opt inline
		return .Equals(dest.negate, l, ASMReg(), exp)


	function EqualsSame (fn_opasm)
		|| num = (dest isnt negate)|int|
		return .NumToReg(exp, num, dest, DataTypeCode.bool).BoolAnswer
	
	
	function Equals (fn_opasm)
		|| rr = r.reg,  || ll = l.reg
		if !rr  and  ll
			swap (l) (r) // put reg 0 into the left... more convenient.
		(Dest  as=  l & r & asmreg.RealConst)
		
		if rr == ll
			return .EqualsSame(dest, l, r, exp)
		
		if dest.IsInt
			rz = .EqualsInt(dest, l, r, exp)
		  else
			rz = .CompareFloat(dest, l, r, exp, 2)						
			
		rz = rz.boolanswer
	
		
	function NotEq  (fn_opasm)
		return .Equals(dest.negate, l, r, exp)


	function KompareIntK (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |uint| Mode, |asmreg|)
		require  l is ConstOutput  or  r is ConstOutput
		|| swapped = l is ConstOutput
			swap (l) (r)					// 10 > x   -->  x <  10  -->  x <= 9
			mode ^= 1						// 10 <= x  -->  x >= 10  -->  x >  9
		|| f = r.fat$

		|| k = f.const - swapped
		require l.small and k.fits(9, true)	
		.NopConst(R)
		k = k|uint64|.trim(9)
		|| J = exp.JMPK(l, k)
		if 0: exp.JMPKN
		j._op += mode&1						// jmpkn
		return J * dest


	function CompareInt (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |int| Mode, |asmreg|)
//		CmpSub(0,  A >   B)
//		CmpSub(1,  A <=  B)
//		CmpSub(2, (u64)A >  (u64)B)
//		CmpSub(3, (u64)A <= (u64)B)
		if mode > 1 // wat?
		mode |= !r.signed<<1
		mode ^= dest is Negate
		dest = dest.BoolAnswer // strips negate off dest

		if l is ConstOutput and r is ConstOutput
			return .ConstCompareInt(dest, L, R, exp, Mode)

		mode |= l.small << 2
		mode |= R.small << 3		

		if Dest isnt CondAnswer
			return exp.CMPI(dest, L, R, mode) * dest

		|| kk = .KompareIntK(dest, l, r, exp, mode)
			return kk
		
		return exp.JMPI(L, R, mode, 0) * dest


	function ConstCompareFloat (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |int| Mode, |asmreg|)
		.Nop2Consts(r, l)
		|| Result = .ConstCompareFloatSub(l, r, mode)
		return .NumToReg(exp, result|int|, dest.BoolAnswer, datatypecode.bool)
		
		
	function ConstCompareFloatSub (|asmreg| L, |asmreg| R, |int| Mode, |bool|)
		opt inline
		ifn mode & 4
			|| A = L.f32
			|| B = R.f32
			if mode <= 1
				if mode == 0
					return A >  B
				return A <= B
			if mode == 2
				return A == B
			return A != B

		|| A = L.f64
		|| B = R.f64
		mode -= 4
		if mode <= 1
			if mode == 0
				return A >  B
			return A <= B
		if mode == 2
			return A == B
		return A != B


	function ConstCompareInt (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |int| Mode, |asmreg|)
		.Nop2Consts(r, l)
		|| Result = .ConstCompareIntSub(l, r, mode)
		return .NumToReg(exp, result|int|, dest.BoolAnswer, datatypecode.bool)
		
		
	function ConstCompareIntSub (|asmreg| L, |asmreg| R, |int| Mode, |bool|)
		opt inline
		|| A = L.Const
		|| B = R.Const
		if mode <= 1
			if mode == 0
				return A >  B
			return A <= B
		if mode == 2
			return A == B
		return A != B


	function CompareFloat (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |int| Mode, |asmreg|)
		mode |= r.IsBig<<2
		// if dest is negate
		mode ^= dest is Negate
		dest = dest.BoolAnswer
		if (l is constoutput and r is constoutput) // needs to handle this!
			return .ConstCompareFloat(dest, L, R, exp, Mode)
		if dest isnt Condanswer
			return exp.CMPF(dest, L, R, Mode) * dest
		return exp.JMPF(L, R, Mode, 0) * dest


	function Compare (|asmreg| dest,  |asmreg| L,  |asmreg| R,  |message| exp,  |int| Mode,  |asmreg|)
/*	 (A >  B)    (A <= B)	*/
		if l.IsInt
			return .CompareInt(dest, l, r, exp, mode)
		return .CompareFloat(dest, l, r, exp, mode)
	
	function More   (fn_opasm)
		return .compare(dest, l, r, exp, 0)

	function LessEq (fn_opasm)
		return .compare(dest, l, r, exp, 1)

	function Less   (fn_opasm)
		return .compare(dest, r, l, exp, 0)

	function MoreEq (fn_opasm)
		return .compare(dest, r, l, exp, 1)



	// MATH 
	function QuickFloat32Plus    (fn_OpASM)
		if  r is constoutput // a + 1,   a - 1
			|| QQ = .QuickFloatPlusConstSub(dest, l, r, exp)
				return qq
		if  l is constoutput  and  l isnt Alternate   // 1 + a --> a + 1
			return .QuickFloatPlusConstSub(dest, r asnt asmreg.alternate, l, exp)
	
			
	function QuickFloatPlusConstSub    (fn_OpASM)
		|| k = r.Const|uint64|
		k ^= (l is Alternate)<<31
		require (k >> 18) << 18 == k
		.NopConst(r)
		if 0: exp.VADK
		return exp.FADK(dest, l, k).Vectorise(dest, asm.vadk)


	function message.DivByZero
		problem (self, "Divide by zero???")	


	function QuickFloatDiv    (fn_OpASM)
		require r is ConstOutput
		|| v = r.float
		|| v2 = v.abs
		// could use exp.FEXK to do powers of 2
		if v2 == 1.0 or 0.0
			if v2 == 0.0
				Exp.DivByZero
			.nopconst(r)
			return .Quick1Or1Sub(dest, l, v|int|, exp)


	// from 1 to -1
	function Quick1Or1Sub (|&Assembler| self, |asmreg| dest, |asmreg| L, |int| ptoi, |message| exp, |asmreg|)
		if ptoi == -1										//   x * -1  -->  0-x 
			return .Subtract(dest, ASMReg(), l, exp)
		if ptoi == 1										//   x * 1   -->  x 
			return l
		return dest.µtype.zero								//   x * 0   -->  0


	function QuickIntMul    (fn_OpASM)
		require r is constoutput
		|| Pow2 = .IntPowerOfTwo(r)
			if Pow2 <= 1
				return .Quick1Or1Sub(dest, l, Pow2, exp)
			if 0: exp.QFLG
			return exp.BFLG(dest, L,  Pow2-1, 0, 0).Vectorise(Dest, asm.QFLG) // y = x * 4  -->  y = x << 2
			
		|| five = .IntPowerOfTwo(r, 1) // 5 or 9 or 17 or 33, etc
			if 0: exp.QADD, exp.QSUB
			return exp.ADD(Dest, L, L, five-1).Vectorise(dest, asm.QADD)

		|| RR = r.const
		if !dest.isvec and .CanAddK(r, RR)								// a - 1,  a + 1
			return exp.MULK(dest, L, RR) * dest


	function QuickFloatMul    (fn_OpASM)
		require r is constoutput	
		|| v = r.float
		if  v == 1.0 or 0.0 or -1.0
			.nopconst(r)
			return .Quick1Or1Sub(dest, l, v|int|, exp)
		if  v == 2.0
			.nopconst(r)
			return .plus(dest, l, l, exp)
		
		if r.Fourbytes
			|| x = r.const
			|| y = x >> 18
			if x == y << 18
				.NopReg(r)
				if 0: exp.VMLK
				return exp.FMLK(dest, l, x).Vectorise(dest, asm.VMLK)

	
	function ASMReg.SomePointer (|bool|)
		return (self is containsaddr) or .µtype.ispointer


	function ASMReg.LeftScore (|int|) // we want pointers on the left... ints on the left, and consts on the right
		return (.somepointer<<1) + (self isnt RealConst)
	
	
	function asmreg.PointerMul (|message| exp, |int| swapped, |int|)
		opt norefcounts
		|| fn = exp.Func
		if fn == @opp
			if swapped
				exp = exp.next!
			  else
				exp = exp.prev!
			
		  elseif fn == @arel or @brel or @acc
			exp = exp.first!
			
		  elseif fn == @dot // obj.Prop // requires pointermul on .prop cos of prev props
			// leave it?
			debugat
		  else
			error "aaargh"
			debugger 
		|| d = exp.ASMDecl.Internal
			return d.CArraySize
		debugger // hmmmm. its not a pointer after all?


	function InlineAddK (|asmreg| L, |int64| R, |asmreg| Dest, |bool|)
		// this may be over-optimisation:   (dest.reg == l.reg) + .SmartSecretFat
		// this allows for optimising x++, x++ --> x+=2
		// but thats all, and it does a lot of effort to achieve this.
		// we can't make the same effort all over for every opt we wanna make
		// Allow as a once-off, to prove a point. Its possible but not the right way.
		if dest.IsVec // now what?
		
		if (l is temp) or (dest.reg == l.reg)
			|| fat = .SmartSecretFat(l)
			if fat isa asm.addk
				|| CC = fat.r2 + R
				if CC.CanStoreAsIntImmediate
					fat.r2 = CC
					fat.r0 = dest
					return true


	function IntPlus     (fn_opasm)
		//		a - 1,		a + 1		//
		//		1 - a,		1 + a		//
		if dest is ConstOutput // perhaps we can do this in domathsub?
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyIntPlus)

		|| swapped = 0
		if  l.LeftScore < r.LeftScore // put consts into R
			if l is alternate
				target debug
					if r.µtype.IsPointer						// 1 - ptr??? we blocked this already.
			  else
				swapped = 1
				swap (l) (r) // a+b == b+a
	
		if !r.reg // add or subtract zero
			return .Assign(dest, l, exp)
		
		|| LMul = 1
		if (l isnt noscale) and l.SomePointer
			lmul = l.PointerMul(exp, swapped)
		if r is ConstOutput
			|| RR = r.const
			if l is alternate
				RR = -RR
			rr *= lmul
			if .CanAddK(r, RR)									// a - 1,  a + 1
				if .InlineAddK(l, rr, dest)
					return dest
				if 0: exp.QADK
				return exp.ADDK(dest, L, RR).Vectorise(dest, asm.QADK)

		if LMul.IsPow2			  								// x = int1 + int2
			// if its a normal add a = b + c, then LMUL==1, which comes through here.
			return exp.ADD_(dest, L, R, LMul.log2) * dest		// x = lptr + r<<3
		
		// L is now a POINTER.
		|| v = .NumToReg(exp,  LMul,  ASMReg(),  DataTypeCode.uint)
		if l is Alternate										// x = (lptr - rptr)/12
			exp.ADD_(dest, L, R, ASMReg())!
			debugger
			return .DivInt(dest, dest, v, exp)

		// L is still a pointer.								// x = lptr + r*12
		return .MUL_(dest, R, V, L, exp) * dest
	
	
	function SmallToBig (|ASMReg| Src, |ASMReg| Cmp, |message| exp, |ASMReg|)
		if src.small and cmp.IsBig
			cmp.reg = 0
			|| dest = .TempOnly(exp, cmp)
			return exp.BFLG(dest, src, 32, 32, 0) * dest
		return src
		

	function FloatPlus (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyFloatPlus)

		|| IsF32 = dest.FourBytes
			|| qq = .QuickFloat32Plus(dest, l, r, exp)
				return qq

		|| ss = (l is alternate)
		|| rr = r.reg
		if 0: exp.VADD
		return exp.fADD(dest, l, r*!ss, r*ss, 1-IsF32).Vectorise(dest, asm.vadd)


	function Plus (fn_opasm) // add (
		|| rr = r.reg, || ll = l.reg
		if !rr
			return l
		if l is alternate
			if rr == ll
				return .zeros(dest)
		  else
			if !ll
				return r
		if l.isint
			return .IntPlus(dest, l, r, exp)
		return .FloatPlus(dest, l, r, exp)


	function Subtract (fn_opasm)				// x = a - b
		(l is alternate)						// the logic ismostly shared...
		return .Plus(dest, l, r, exp)


	function Minus (fn_opasm)					// x = -b
		// not the same as negative, which is an fn_asm... that means it works on the exp
		if r != asmreg()
		(r is alternate)
		return .plus(dest, r, l, exp)


	function FloatMul (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyFloatMul)
		rz = .QuickFloatMul(dest, R, L, exp)
		if rz == nil
			rz = .QuickFloatMul(dest, L, R, exp)
			if rz == nil
				if 0: exp.VMUL
				rz = exp.FMUL(dest, L, R, asmreg(), l.IsBig|int|).Vectorise(dest, asm.vmul)


	function IntMul (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyIntMul)
		rz = .QuickIntMul(dest, R, L, exp)
		if rz == nil
			rz = .QuickIntMul(dest, L, R, exp)
			if rz == nil
				if 0: exp.qmul
				rz = .MUL_(dest, L, R, asmreg(), exp).Vectorisesmall(dest, asm.qmul, asm.muls)


	function BoolMul (|asmreg| dest, |asmreg| Bule, |asmreg| V, |message| exp, |asmreg|)		
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, Bule, V, exp, ConstifyBoolMul)
		rz = .QuickIntMul(dest, Bule, V, exp)
		if !rz
			rz = exp.TERN(dest, Bule, V, asmreg(), bule.small) * Dest				// TERN


	function Multiply (fn_opasm)
		if l isa DataTypeCode.bool
			return .BoolMul(dest, l, r, exp)
		if r isa DataTypeCode.bool
			return .BoolMul(dest, r, l, exp)

		if l is ConstOutput
			swap (l) (r)
		if l.isint
			return .IntMul(dest, r, l, exp)
		return .floatmul(Dest, l, r, exp)
	

	function Divide (fn_opasm)
		if r.reg == l.reg											// return 1
			return .SelfDivide(dest, exp)
		if dest.isint
			return .DivInt(dest, l, r, exp)
		return .DivFloat(dest, l, r, exp)


	function DivFloat    (fn_opasm)
		if (dest is ConstOutput) and (l isnt alternate)
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyFloatDiv)
		rz = .QuickFloatDiv(dest, L, R, exp)
		if rz == nil
			if 0: exp.VDIV
			rz = exp.FDIV(dest,  L,  R,  asmreg(), dest.isbig|int|).Vectorise(dest, asm.VDIV)


	function SelfDivide (|asmreg| dest, |message| exp, |asmreg|)
		if dest.IsInt
			return .NumToReg(exp, 1, dest, datatypecode.int) // x/x=1
		if dest.small
			return .NumToReg(exp, 0x3f800000 /*1.0*/, dest, DataTypeCode.float)
		return .NumToReg(exp, 0x3ff0000000000000 /*1.0 double*/, dest, DataTypeCode.f64)


	function QuickIntDiv    (fn_opasm)
		require r is ConstOutput
		|| PToi = .IntPowerOftwo(r)$
		if ptoi <= 1																//   x/1  x/0  x/-1
			if ptoi == 0
				exp.DivByZero
			return .Quick1Or1Sub(dest, l, ptoi, exp)
		if !l.Signed
			return exp.BFLG(dest, l, 0, ptoi-1, 0).Vectorise(Dest, asm.QFLG)		// u / 2 --> u >> 1
		return exp.DIV2(dest, l, l.Bitloss, ptoi-1) * Dest		// nice


	function DivInt    (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, L, R, exp, ConstifyIntDiv)
		|| q = .QuickIntDiv(dest, l, r, exp)
			return q

		// Dividing a big one by a small one, then?     bad.   We use 64-bit divide. But the small has garbage

		if l.isbig and dest.small		// what?		
		if l.IsBig and r.small         // Sigh.  Small has garbage, but were doing a 64-bit divide.
			r = .SmallToBig(r, l, exp)
		
		if 0
			exp.DIVS(ASMReg(), ASMReg(), ASMReg(), ASMReg(), 0)
			exp.QDIV(ASMReg(), ASMReg(), ASMReg(), ASMReg(), 0)
		return exp.DIV(dest, ASMReg(), l, r, dest.signed|int|).Vectorisesmall(dest, asm.qdiv, asm.divs)


	function ModInt    (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, L, R, exp, ConstifyIntMod)
		return exp.DIV(asmreg(), dest, L, R, dest.signed|int|) * Dest


	function ModFloat    (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, L, R, exp, Constifyfloatmod)
		|| big = dest.IsBig|int|
		exp.fdiv(dest, L, R, asmreg(), big).Vectorise(dest, asm.VDIV)
		if 0: exp.vfrc
		return exp.FFRC(dest, asmreg(), dest, R, big).Vectorise(dest, asm.VFRC)

	
	function Mod    (fn_opasm)
		if dest.isint
			return .ModInt(dest, l, r, exp)
		return .ModFloat(dest, l, r, exp)


	// BITS
	function BitNot (fn_opasm)
		if r.reg // what?
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, asmreg(), exp, ConstifyBitNot)
		if 0: exp.BNOT, exp.qnot
		return .BitMaker(dest, l, asmreg(), exp, asm.bnot, asm.qnot)


	function BitXor (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyBitXor)
		if (l.reg == r.reg)
			return .Zeros(dest)
		if !l.reg
			return r
		if !r.reg
			return l
		if (l is ConstOutput)
			swap (l) (r)
		if 0: exp.BXOR, exp.qxor
		return .BitMaker(dest, l, r, exp, asm.bxor, asm.qxor)


	function BitOr (fn_opasm)
		if !dest.reg
		
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyBitOr)
		
		|| ll = l.reg
		|| rr = r.reg

		if dest isnt Textual
			if (ll == rr)
				return l
			if l is constoutput
				if !l.reg
					return r
				if l.isconst(-1)
					return l
			if r is constoutput
				if !r.reg
					return l
				if r.isconst(-1)
					return r
		if (l is ConstOutput)
			swap (l) (r)
		if 0: exp.BORR, exp.qorr
		return .BitMaker(dest, l, r, exp, asm.borr, asm.qorr)

			
	function BitAnd (fn_opasm)		
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyBitAnd)
		|| ll = l.reg
		|| rr = r.reg
		if (ll == rr)
			return l
		if !ll or !rr
			return .Zeros(dest)
		if .NopMinusOne(l)
			return r
		if .NopMinusOne(r)
			return l
		if (l is ConstOutput)
			swap (l) (r)
		if 0: exp.qand
		return .BitMaker(dest, l, r, exp, asm.band, asm.QAND)
	
	
	function BitMaker (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |asm| Opp, |asm| VOpp,   |asmreg|)
		if asm.BORRK!=asm.borr+4 // what?

		if r.reg and (opp != asm.bnot) and !dest.IsVec
			|| k = .Const(R, 14, true)
				if 0:  exp.BORRK,  exp.BXORK
				|| fat = exp.BANDK(dest, L, K) 
				fat._op = opp+4
				return fat * Dest
				
		|| Clear = l.BitCount - r.BitCount
		if Clear < 0
			swap (l) (r)
			clear = -clear
		
		if Clear and r.signed and l.reg
			r = exp.ExpandBits(r, l.µtype) // wat a shame.
			clear = 0
			
		|| fat = exp.BAND(dest, L, R, Clear)
		fat._op = opp
		return fat * Dest


	function SHR  (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, r, exp, (ConstifyBitSHS, ConstifyBitSHR)(l.signed))
		|| K = .const(R, 6, false)
			return .BFLG_Const(exp, dest, l, 0, k)
		
		|| fat = exp.BRSH(dest, L, R, l.bitloss)
		fat.Vectorise(dest, asm.QRSS)
		if l.Signed
			if 0: exp.brss, exp.QRSS, exp.QRSH
			fat._op--
		return fat * dest	


	function SHL  (fn_opasm)
		if dest is ConstOutput
			return .MakeConstFromTwo(dest, l, r, exp, ConstifyBitSHL)
		|| K = .const(R, 6, false)
			return .BFLG_Const(exp, dest, l,  K, 0)
		
		if 0: exp.QLSH
		return exp.BLSH(dest, L, R, l.bitloss).Vectorise(Dest, asm.QLSH)


	function BFLG_Const  (|message| exp, |asmreg| dest, |asmreg| Src, |uint| up, |uint| down, |asmreg|)
		|uint| SrcBits = src.BitCount
		if up > srcbits or down > SrcBits			// wierd?
			problem (exp, "Shift too far")
		if !up and !down
			return .assign(dest, src, exp)
		
		|| extra = 64 - srcBits // handle extra bits.
		up += extra
		down += extra
		if  !src.reg  or  (up >= 64)  or  (!src.signed and down >= 64)
			return .zeros(dest)
	
		return exp.BFLG(dest,  src,  up,  down,  src.signed|int|) * dest


	function Zeros (|asmreg| dest, |asmreg|)
		rz = asmreg()
		(rz is constinput) = (dest is ConstInput)
		rz.µtype = dest.µtype
	

	function CanImproveAssign (|asmreg| dest, |asmreg| src, |fatasm| f, |asmreg|)
		opt inline
		if .IsCurr(f)
			if (src is temp) and (f isa asm.bflg)
/				f.prms[0] = dest.reg
				return dest
	
	
	function Assign (|asmreg| dest, |asmreg| src, |message| exp, |asmreg|)  // assigns (		
		|| sr = src.reg
		|| dr = dest.reg
		if sr == dr
// if we wanted to... we could make this stricter, just like the test below... that does bflg.
// for now leave as is.
// we could "unstricten" it by using f.isread
			return dest

		if (dest.IsVec != src.IsVec) and src.reg
		if dest.IsVec
			return exp.VMOV(dest, src, asmreg(), asmreg()) * dest
		
		if !sr										// must be assigning zero
			return .LoadNumber(exp, 0, dest)		// More readable ASM.

		|| f = src.fat
			|| improve = .CanImproveAssign(dest, src, f)
				return improve

		
		// BFLG is VERY optimisable :]
		// prefered op for most things. Chainability of optimisations is very good.
		
		dest = dest.CopyConst(Src)
		|| sh = src.BitCount - dest.BitCount
		if sh >= 0
			// leave as is
		  elseif src.signed
			sh = 64 - src.bitcount
		  else
			sh = 0 // no point otherwise. just make the asm simpler.
		
		return exp.BFLG(dest, src, sh, sh, src.signed|int|) * Dest


	function NopMinusOne (|asmreg| r, |bool|)
		if r.IsConst(-1)
			.nopconst(r)
			return true
	
	
function int64.Fits (|int| amount, |bool| signed=true,  |bool|)
	|int64| sh = 64 - amount
	|| x = self << sh
	if signed
		x >>= sh
	  else
		x = X|uint64| >> sh
	return x == self

