

prototype fn_OpASM          (|&Assembler| self, |message| exp, |asmreg| dest, |asmreg| L, |asmreg| R, |asmreg|)
prototype fn_ASMConstifier  (|asmreg| L, |asmreg| R, |int64|)



extend asmreg
	function BoolAnswer (|asmreg|)
		.µtype = DataTypeCode.bool
		
		is AlreadyNegated
		isnt negate
		if self is CondRequest
			isnt condrequest
			is CondAnswer
		return self 

	
	
extend Assembler
	function DoMath (fn_asm)
		opt norefcounts
		|| op = exp.second!
		|| scop = op.obj|scoperator|!
		if scop is AndOr
			return .ASMBoolMaker(exp, dest, scop.Kind)
		|| fn = scop.OpASM!
		return .DoMathSub(exp, dest, fn)


	helper DoMathSub (|message| exp,  |ASMReg| Mode,  |fn_opasm!| fn, |ASMReg|)
		opt norefcounts
		|| ml = ASMReg()
		|| mr = ASMReg()
		|| First = exp.first!
		|| dest = mode
		target debug: || mathtrap = ++ASMTrapper, if mathtrap == -1
		if First.islast
			ml = .µGetASM(first, dest)
		  else
			ml = .µGetRealOffer(First, dest)
			if (dest is UnusedSoFar) and !(ml iz dest)
				mr = dest
	
			mr = .µ(exp.last!, mr)
			if ml.IsVec != mr.IsVec
				if ml.IsVec
					mr = .VectorUpgrade(mr, exp)
				  else
					ml = .VectorUpgrade(ml, exp)
		
		if dest is Discard
			.AskNopTemp(ml)
			.AskNopTemp(mr)
			return dest // returning from inline, into func that discards result.
		
		dest = .TempTyped(exp, dest)
		(dest is Const) = (ml&mr is Const)
		
		|| op = exp.second ?? exp.first!
		target debug: || mathtrap2 = ++asmtrapper, if mathtrap2 == -1
		rz = (fn)(self, op, dest, ml, mr)
		
		target debug: if rz.IsVec != ml.IsVec			// argh?
		
		rz = rz.ExpectSameType(dest)


	function EqualsInt (|message| exp, |asmreg| dest, |asmreg| L, |asmreg| R, |asmreg|)
		|| negate = dest is negate
		|| Res = dest.BoolAnswer						// Hmmm... so what should it be?
		if  L is Const  and  R is Const
			.Nop2Consts(r, l)
			|| num = ((l.const==r.const)!=negate)|int|
			return .NumToReg(exp, res, num, DataTypeCode.bool)
		
		if dest isnt CondRequest
			dest = .TempTyped(TypeBool!.TypeNormal, dest)
			return exp.EQUL(dest, L, R, negate.equlmode(l,r)) * dest

		if  r is Const  and  l isnt Const
			swap (l) (r)
		
		|| RS = r.small|int|
		|&fatasm| fat
		if !l.IsZero
			|| LS = L.Small|int|
			if RS
				|| K = .Const(L, 9, L.signed)
					fat = exp.JMKE(R, K, 0)
					if negate
						fat._op = asm.JMKN
			if !fat
				fat = exp.JMPN(l, r, LS, RS, 0)
				if !negate
					fat._op = asm.jmpe
		  else
			fat = exp.JBAN(r, RS, 0)
			if !negate
				fat._op = asm.jbor
		if 0: exp.JMKN, exp.JMPE, exp.JBOR
		return fat * res
	
	
	function Exists (|ASMReg| dest, |ASMReg| L, |message| exp, |ASMReg|)
		opt inline
		return .Equals(exp, dest.negate, l, ASMReg())


	function EqualsSame (fn_opasm)
		|| num = (dest isnt negate)|int|
		return .NumToReg(exp, dest, num, DataTypeCode.bool).BoolAnswer
	
	
	function Equals (fn_opasm)
		|| rr = r.reg,  || ll = l.reg
		if !rr  and  ll
			swap (l) (r) // put reg 0 into the left... more convenient.
		(Dest  as=  l & r & asmreg.Const)
		
		if rr == ll
			return .EqualsSame(exp, dest, l, r)
		
		if dest.IsInt
			rz = .EqualsInt(exp, dest, l, r)
		  else
			rz = .CompareFloat(dest, l, r, exp, 2)						
			
		rz = rz.boolanswer
	
		
	function NotEq  (fn_opasm)
		return .Equals(exp, dest.negate, l, r)


	function KompareIntK (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |uint| Mode, |asmreg|)
		require  l is Const  or  r is Const
		|| swapped = l is Const
			swap (l) (r)					// 10 > x   -->  x <  10  -->  x <= 9
			mode ^= 1						// 10 <= x  -->  x >= 10  -->  x >  9
		|| f = r.fat$

		|| k = f.const - swapped
		require l.small and k.fits(9, true)	
		.NopConst(R)
		k = k|uint64|.trim(9)
		|| J = exp.JMKM(l, k)
		if 0: exp.JMKL
		j._op += !(mode&1)						// .JMPL
		return J * dest


	function CompareInt (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |int| Mode, |asmreg|)
//		CmpSub(0,  A >   B)
//		CmpSub(1,  A <=  B)
//		CmpSub(2, (u64)A >  (u64)B)
//		CmpSub(3, (u64)A <= (u64)B)
		if mode > 1 // wat?
		mode |= !r.signed<<1
		mode ^= dest is Negate
		dest = dest.BoolAnswer // strips negate off dest

		if l is Const and r is Const
			return .ConstCompareInt(dest, L, R, exp, Mode)

		mode |= l.small << 2
		mode |= R.small << 3		

		if Dest isnt CondAnswer
			return exp.CMPI(dest, L, R, mode) * dest

		|| kk = .KompareIntK(dest, l, r, exp, mode)
			return kk
		
		return exp.JMPI(L, R, mode, 0) * dest


	function ConstCompareFloat (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |int| Mode, |asmreg|)
		.Nop2Consts(r, l)
		|| Result = .ConstCompareFloatSub(l, r, mode)
		return .NumToReg(exp, dest.BoolAnswer, result|int|, datatypecode.bool)
		
		
	function ConstCompareFloatSub (|asmreg| L, |asmreg| R, |int| Mode, |bool|)
		ifn mode & 4
			|| A = L.f32
			|| B = R.f32
			if mode <= 1
				if mode == 0
					return A >  B
				return A <= B
			if mode == 2
				return A == B
			return A != B

		|| A = L.f64
		|| B = R.f64
		mode -= 4
		if mode <= 1
			if mode == 0
				return A >  B
			return A <= B
		if mode == 2
			return A == B
		return A != B


	function ConstCompareInt (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |int| Mode, |asmreg|)
		.Nop2Consts(r, l)
		|| Result = .ConstCompareIntSub(l, r, mode)
		return .NumToReg(exp, dest.BoolAnswer, result|int|, datatypecode.bool)
		
		
	function ConstCompareIntSub (|asmreg| L, |asmreg| R, |int| Mode, |bool|)
		|| A = L.Const
		|| B = R.Const
		if mode <= 1
			if mode == 0
				return A >  B
			return A <= B
		if mode == 2
			return A == B
		return A != B


	function CompareFloat (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |int| Mode, |asmreg|)
		mode |= r.IsBig<<2
		// if dest is negate
		mode ^= dest is Negate
		dest = dest.BoolAnswer
		if (l is Const and r is Const) // needs to handle this!
			return .ConstCompareFloat(dest, L, R, exp, Mode)
		if dest isnt Condanswer
			return exp.CMPF(dest, L, R, Mode) * dest
		return exp.JMPF(L, R, Mode, 0) * dest


	function Compare (|asmreg| dest,  |asmreg| L,  |asmreg| R,  |message| exp,  |int| Mode,  |asmreg|)
/*	 (A >  B)    (A <= B)	*/
		if l.IsInt
			return .CompareInt(dest, l, r, exp, mode)
		return .CompareFloat(dest, l, r, exp, mode)
	
	function More   (fn_opasm)
		return .compare(dest, l, r, exp, 0)

	function LessEq (fn_opasm)
		return .compare(dest, l, r, exp, 1)

	function Less   (fn_opasm)
		return .compare(dest, r, l, exp, 0)

	function MoreEq (fn_opasm)
		return .compare(dest, r, l, exp, 1)



	// MATH 
	function QuickFloat32Plus    (fn_OpASM)
		if  r is Const // a + 1,   a - 1
			|| QQ = .QuickFloatPlusConstSub(exp, dest, l, r)
				return qq
		if  l is Const  and  l isnt Alternate   // 1 + a --> a + 1
			return .QuickFloatPlusConstSub(exp, dest, r asnt asmreg.alternate, l)
	
			
	function QuickFloatPlusConstSub    (fn_OpASM)
		|| k = r.Const|uint64|
		k ^= (l is Subtract)<<31
		require (k >> 18) << 18 == k
		.NopConst(r)
		if 0: exp.VADK
		return exp.FADK(dest, l, k).Vectorise(dest, asm.vadk)


	function message.DivByZero
		problem (self, "Divide by zero???")	


	function QuickFloatDiv    (fn_OpASM)
		require r is Const
		|| v = r.float
		|| v2 = v.abs
		// could use exp.FEXK to do powers of 2
		if v2 == 1.0 or 0.0
			if v2 == 0.0
				Exp.DivByZero
			.nopconst(r)
			return .Quick1Or1Sub(dest, l, v|int|, exp)


	// from 1 to -1
	function Quick1Or1Sub (|&Assembler| self, |asmreg| dest, |asmreg| L, |int| ptoi, |message| exp, |asmreg|)
		if ptoi == -1										//   x * -1  -->  0-x 
			return .Subtract(exp, dest, ASMReg(), l)
		if ptoi == 1										//   x * 1   -->  x 
			return l.µtype(dest)
		return dest.µtype.zero								//   x * 0   -->  0
	
	
	function QuickIntMul    (fn_OpASM)
		require r is Const
		|| Pow2 = .IntPowerOfTwo(r)
			if Pow2 <= 1
				return .Quick1Or1Sub(dest, l, Pow2, exp)
			if 0: exp.QFLG
			return exp.BFLG(dest, L,  Pow2-1, 0, 0).Vectorise(Dest, asm.QFLG) // y = x * 4  -->  y = x << 2
			
		|| five = .IntPowerOfTwo(r, 1) // 5 or 9 or 17 or 33, etc
			if 0: exp.QADD, exp.QSUB
			return exp.ADD(Dest, L, L, five-1).Vectorise(dest, asm.QADD)

		|| RR = r.const
		if !dest.isvec and .CanAddK(r, RR)								// a - 1,  a + 1
			return exp.MULK(dest, L, RR) * dest


	function QuickFloatMul    (fn_OpASM)
		require r is Const	
		|| v = r.float
		if  v == 1.0 or 0.0 or -1.0
			.nopconst(r)
			return .Quick1Or1Sub(dest, l, v|int|, exp)
		if  v == 2.0
			.nopconst(r)
			return .plus(exp, dest, l, l)
		
		if r.Fourbytes
			|| x = r.const
			|| y = x >> 18
			if x == y << 18
				.NopReg(r)
				if 0: exp.VMLK
				return exp.FMLK(dest, l, x).Vectorise(dest, asm.VMLK)

	
	function ASMReg.SomePointer (|bool|)
		return (self is containsaddr) or .µtype.ispointer


	function ASMReg.LeftScore (|int|) // we want pointers on the left, and consts on the right
		rz += .SomePointer << 2				// pointers on left
		rz += (self isnt Const) << 1	// consts on right
		rz += self isnt temp				// temps on right
	
	
	function asmreg.PointerMul (|message| exp, |bool| swapped, |int|)
		opt norefcounts
		|| d = .PointerMulSub(exp, swapped)
			return d.CArraySize
		debugger // hmmmm. its not a pointer after all?


	function asmreg.PointerMulSub (|message| exp, |bool| swapped, |scdecl|)
		opt norefcounts
		|| fn = exp.Func
		if fn == @opp
			exp = exp.step(swapped)!
			
		  elseif fn == @arel or @brel or @acc
			exp = exp.first!
			
		  else
			if fn == @dot // obj.Prop // requires pointermul on .prop cos of prev props
				return exp.asmdecl
			error "aaargh"
			debugger
		
		return exp.ASMDecl.Internal


	function InlineAddK (|asmreg| XIn, |int64| Add, |asmreg| XOut, |bool|)
//		x = y + 1
//		x = x + 1
//			-->
//		x = y + 2


//		t = y + 1 // t is a temp.
//		x = t + 1
//			-->
//		x = y + 2

		|| XY1 = .Last(asm.addk)			#require
		if XOut.IsVec // now what?
		|| xinreg = XIn.reg
		require xinreg == XY1.RegOnly
		|| Changed = xinreg != xout.reg
			require (xin is temp)
		|| add2 = XY1.prms[2] + add		
		if add2.CanStoreAsAddK
			xy1.r2 = add2
			if Changed
				.ReDest(xy1, xout)
			return true
		
		
	function IntPlus     (fn_opasm)
		if dest is Const									// perhaps we can do this in domathsub?
			return .MakeConstFromTwo(exp, dest, l, r, ConstifyIntPlus)
		
		|| swapped =  l isnt alternate  and  l.LeftScore < r.LeftScore
			swap (l) (r) // a+b == b+a
	
		if r.iszero // add or subtract zero
			return l//.Assign(exp, dest, l)

		|| LMul = 1
		if (l isnt noscale) and l.SomePointer
			lmul = l.PointerMul(exp, swapped)
		if r is Const
			|| RR = r.const
			if l is Subtract
				RR = -RR
			rr *= lmul
			if .CanAddK(r, RR)									// a - 1,  a + 1
				if .InlineAddK(l, rr, dest)
					return dest
				if 0: exp.QADK
				return exp.ADDK(dest, L, RR).Vectorise(dest, asm.QADK)

		if LMul.IsPow2			  									// x = int1 + int2
			return exp.AddOrSub(dest, L, R, LMul.log2) * dest		// x = lptr + r<<3
		
		// L is now a POINTER.
		|| v = .NumToReg(exp,  ASMReg(),  LMul,  DataTypeCode.uint)
		if l is Subtract										// x = (lptr - rptr)/12
			exp.AddOrSub(dest, L, R, 0)!
			debugger
			return .DivInt(exp, dest, dest, v)

		// L is still a pointer.								// x = lptr + r*12
		return .MUL_(dest, R, V, L, exp) * dest
	
	
	function SmallToBig (|ASMReg| Src, |ASMReg| Cmp, |message| exp, |ASMReg|)
		if src.small and cmp.IsBig
			cmp.reg = 0
			|| dest = .TempOnly(cmp)
			return exp.BFLG(dest, src, 32, 32, 0) * dest
		return src
		

	function FloatPlus (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, r, ConstifyFloatPlus)

		|| IsF32 = dest.FourBytes
			|| qq = .QuickFloat32Plus(exp, dest, l, r)
				return qq

		|| ss = (l is Subtract)
		if 0: exp.VADD
		return exp.fADD(dest, l, r*!ss, r*ss, 1-IsF32).Vectorise(dest, asm.vadd)


	function Plus (fn_opasm) // add (
//		(dest is Const) = (l&r) is Const
		rz = .Plussub(exp, dest, l, r) // into won't be passed in anymore. So just remove the clear?
		rz = rz asnt asmreg.subtract


	function PlusSub (fn_opasm) // add (
		if r.iszero					// allow a = a ± 0
			return l.µtype(dest)
		
		if l isnt Subtract			// 0 + a
			if l.iszero
				return r.µtype(Dest)
		  else
			if (r iz l)
				return .zeros(exp, dest)
		
		if l.IsFloat
			return .FloatPlus(exp, dest, l, r)
		return .IntPlus(exp, dest, l, r)


	function Subtract (fn_opasm)				// x = a - b
		(l is Subtract)							// the logic ismostly shared...
		return .Plus(exp, dest, l, r)


	function Minus (fn_opasm)					// x = -b
		// not the same as negative, which is an fn_asm... that means it works on the exp
		if r != asmreg()
		(r is Subtract)
		return .plus(exp, dest, r, l) asnt asmreg.Alternate


	function FloatMul (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, r, ConstifyFloatMul)
		rz = .QuickFloatMul(exp, dest, R, L)
		if rz == nil
			rz = .QuickFloatMul(exp, dest, L, R)
			if rz == nil
				if 0: exp.VMUL
				rz = exp.FMUL(dest, L, R, asmreg(), l.IsBig|int|).Vectorise(dest, asm.vmul)


	function IntMul (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, r, ConstifyIntMul)
		rz = .QuickIntMul(exp, dest, R, L)
		if rz == nil
			rz = .QuickIntMul(exp, dest, L, R)
			if rz == nil
				if 0: exp.qmul, exp.muls
				rz = .MUL_(dest, L, R, asmreg(), exp).Vectorisesmall(dest, asm.qmul, asm.muls)


	function BoolMul (|asmreg| dest, |asmreg| Bule, |asmreg| V, |message| exp, |asmreg|)		
		if dest is Const
			return .MakeConstFromTwo(exp, dest, Bule, V, ConstifyBoolMul)
		rz = .QuickIntMul(exp, dest, Bule, V)
		if !rz
			rz = exp.TERN(dest, Bule, V, asmreg(), bule.small|int|) * Dest				// TERN


	function Multiply (fn_opasm)
		if l isa DataTypeCode.bool
			return .BoolMul(dest, l, r, exp)
		if r isa DataTypeCode.bool
			return .BoolMul(dest, r, l, exp)

		if l is Const
			swap (l) (r)
		if l.isint
			return .IntMul(exp, dest, r, l)
		return .floatmul(exp, Dest, l, r)
	

	function Divide (fn_opasm)
		if r iz l												// return 1
			return .SelfDivide(dest, exp)
		if dest.isint
			return .DivInt(exp, dest, l, r)
		return .DivFloat(exp, dest, l, r, )


	function DivFloat    (fn_opasm)
		if (dest is Const) and (l isnt alternate)
			return .MakeConstFromTwo(exp, dest, l, r, ConstifyFloatDiv)
		rz = .QuickFloatDiv(exp, dest, L, R)
		if rz == nil
			if 0: exp.VDIV
			rz = exp.FDIV(dest,  L,  R,  asmreg(), dest.isbig|int|).Vectorise(dest, asm.VDIV)


	function SelfDivide (|asmreg| dest, |message| exp, |asmreg|)
		if dest.IsInt
			return .NumToReg(exp, dest, 1, datatypecode.int) // x/x=1
		if dest.small
			return .NumToReg(exp, dest, 0x3f800000 /*1.0*/, DataTypeCode.float)
		return .NumToReg(exp, dest, 0x3ff0000000000000 /*1.0 double*/, DataTypeCode.f64)


	function QuickIntDiv    (fn_opasm)
		require r is Const
		|| PToi = .IntPowerOftwo(r)$
		if ptoi <= 1																//   x/1  x/0  x/-1
			if ptoi == 0
				exp.DivByZero
			return .Quick1Or1Sub(dest, l, ptoi, exp)
		if !l.Signed
			return exp.BFLG(dest, l, 0, ptoi-1, 0).Vectorise(Dest, asm.QFLG)		// u / 2 --> u >> 1
		return exp.DIV2(dest, l, l.Bitloss, ptoi-1) * Dest		// nice


	function DivInt    (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, L, R, ConstifyIntDiv)
		|| q = .QuickIntDiv(exp, dest, l, r)
			return q

		// Dividing a big one by a small one, then?     bad.   We use 64-bit divide. But the small has garbage

		if l.isbig and dest.small		// what?		
		if l.IsBig and r.small         // Sigh.  Small has garbage, but were doing a 64-bit divide.
			r = .SmallToBig(r, l, exp)
		
		if 0
			exp.DIVS
			exp.QDIV
		|| fat = exp.DIV(dest, ASMReg(), l, r, dest.signed|int|)
		fat._Outputs &= ~2 // clear this output
		return fat.Vectorisesmall(dest, asm.qdiv, asm.divs)


	function ModInt    (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, L, R, ConstifyIntMod)
		|| fat = exp.DIV(asmreg(), dest, L, R, dest.signed|int|)
		fat._Outputs &= ~1 // clear unused output 
		return fat.Vectorisesmall(dest, asm.qdiv, asm.divs)


	function ModFloat    (fn_opasm)
/		if dest is Const
			return .MakeConstFromTwo(exp, dest, L, R, Constifyfloatmod)
		|| big = dest.IsBig|int|
		exp.fdiv(dest, L, R, asmreg(), big).Vectorise(dest, asm.VDIV)
		if 0: exp.vfrc
/		return exp.FFRC(dest, asmreg(), dest, R, big).Vectorise(dest, asm.VFRC)

	
	function Mod    (fn_opasm)
		if dest.isint
			return .ModInt(exp, dest, l, r)
		return .ModFloat(exp, dest, l, r)


	// BITS
	function BitNot (fn_opasm)
		if r.reg // what?
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, asmreg(), ConstifyBitNot)
		if 0: exp.BNOT, exp.qnot
		return .BitMaker(dest, l, asmreg(), exp, asm.bnot, asm.qnot)


	function BitXor (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, r, ConstifyBitXor)
		if (l iz r)
			return .Zeros(exp, dest)
		if l.iszero
			return r
		if r.iszero
			return l
		if (l is Const)
			swap (l) (r)
		if 0: exp.BXOR, exp.qxor
		return .BitMaker(dest, l, r, exp, asm.bxor, asm.qxor)


	function MiniBOR (|asmreg| l, |asmreg| r, |asmreg|)
		if (l iz r)
			return l
		if r is Const
			|| K = r.const
			if k == 0
				return l
			if k == -1
				return r


	function RemoveableBFLG (|asmreg| dest, |asmreg| A,  |asmreg| B, |asmreg|) // canremove, cantemp, caninline, CanSquish
		require (A is temp) or (A iz dest)
		|| bflg = A.Fat$
		require (bflg isa asm.bflg) and (bflg.µRefCount <= 1)
		require (bflg == .Last) and .IsCurr(bflg)
		|| up = bflg.prms[2]|int|
		|| down = bflg.prms[3]
		require (down == 0) or ((64 - A.BitCount) >= up)
		|| Diff = up - down
		if Diff > 0
			bflg.RegInput(2) = bflg.ASMReg(1)
			bflg.RegInput(1) = B
			bflg.prms[3] = Diff
			bflg.prms[4] = 0
			bflg._op = asm.BFLS
			return .ReDest(bflg, dest)


	function MiniBFLS (|&Assembler| self, |message| exp, |asmreg| dest, |asmreg| L, |asmreg| R, |asmreg|)
		|| bl = .RemoveableBFLG(dest, L, R)
			return bl
		|| br = .RemoveableBFLG(dest, R, L)
			return br
			

	function BitOr (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, r,  ConstifyBitOr)

		if (l is Const)
			swap (l) (r)

		if dest isnt Textual
			|| old = .MiniBOR(l,r)
				return old.µtype(Dest)
			|| mini = .MiniBFLS(exp, dest, l, r)
				return mini

		if 0: exp.BORR, exp.qorr
		return .BitMaker(dest, l, r, exp, asm.borr, asm.qorr)

	
	function BitAndNop (|assembler| self, |message| exp, |asmreg| dest,  |asmreg| L, |asmreg| R, |asmreg|)
		require l is Const
		|| k = l.const
		if .nopconst(l, -1)
			return r
		if k == 0
			return .zeros(exp, dest)

	function BitAnd (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, r, ConstifyBitAnd)
		if (l iz r)
			return l
		|| k = .BitAndNop(exp, dest, l, r)
			return k
		k = .BitAndNop(exp, dest, r, l)
		if k
			return k
		if (l is Const)
			swap (l) (r)
		if 0: exp.qand
		return .BitMaker(dest, l, r, exp, asm.band, asm.QAND)
	
	
	function BitMaker (|asmreg| dest, |asmreg| L, |asmreg| R, |message| exp, |asm| Opp, |asm| VOpp, |asmreg|)
		if asm.BORK!=asm.borr+4 // what?

		if r.reg and (opp != asm.bnot) and !dest.IsVec
			|| k = .Const(R, 14, true)
				if 0:  exp.BORK,  exp.BXRK
				|| fat = exp.BANK(dest, L, K) 
				fat._op = opp+4
				return fat * Dest
				
		|| Clear = l.BitCount - r.BitCount
		if Clear < 0
			swap (l) (r)
			clear = -clear
		
		if Clear and r.signed and !l.iszero
			r = exp.ExpandBits(r, l.µtype) // wat a shame.
			clear = 0
			
		|| fat = exp.BAND(dest, L, R, Clear)
		fat._op = opp
		return fat.Vectorise(dest, vopp)


	function BFLG_Right  (|message| exp, |asmreg| dest, |asmreg| Src, |uint| down, |asmreg|)
		// x = y << 1				// doesn't care about sign
		// x = x >> 1				// does
		// -->
		// bflg: x, y, 1, 1
/*
	x = (x << 9)			>> 5
	x = (x << 9) >> 7       >> 4
	x = (x >> 7)            >> 3
*/

		|| xy1 = .last(asm.bflg)$ // what about qflg. allow both?
		require xy1.RegOnly == src.reg
		require (src is temp) or (dest iz src)

		// src is temp:     dest = (will_be_temp<<3) >> kDown
		// dest iz src:     dest = y << 3,  dest = dest >> 3
		
		|| sign = xy1.Prms[4] > 0 // dest0, src1, up2, down3, sign4
		require (sign == src.signed) or !xy1.prms[3]
		
		xy1.prms[3] += down
		if src is temp
			.ReDest(xy1, dest, 0)
		return dest
		
		
		
	function SHR  (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, r, (ConstifyBitSHS, ConstifyBitSHR)(l.signed))
		|| K = .const(R, 6, false)
			if k == 0
				return L
			|| Opt = .BFLG_Right(exp, dest, l, k)
				return Opt
			return .BFLG(exp, dest, l, 0, k)
		
		|| fat = exp.BRSH(dest, L, R, l.bitloss)
		fat.Vectorise(dest, asm.QRSS)
		if l.Signed
			if 0: exp.brss, exp.QRSS, exp.QRSH
			fat._op--
		return fat * dest	


	function SHL  (fn_opasm)
		if dest is Const
			return .MakeConstFromTwo(exp, dest, l, r, ConstifyBitSHL)
		|| K = .const(R, 6, false)
			return .BFLG(exp, dest, l,  K, 0)
		
		if 0: exp.QLSH
		return exp.BLSH(dest, L, R, l.bitloss).Vectorise(Dest, asm.QLSH)


	function MergeBFLG  (|message| exp, |asmreg| dest, |asmreg| Src, |uint| up, |uint| down, |fatasm| bflg,  |asmreg|)
		require ((src is temp) or ((src iz dest) and .iscurr(bflg)))
		// if this test fails... we can STILl opt this! Just take it's input, and make a new µop
		// in fact, we could make a new op always, if its from a bflg.
		// allow for the input to be nopped. For example, inline type-casted params, that are now unused.
		
		|| oup = bflg.prms[2]
		|| odw = bflg.prms[3]
		require oup == odw and oup >= up and oup >= down // sigh... i can't figure out much else right now.
		if oup
			// this logic doesn't make perfect sense either. why add only if its not 0?
			|| total = up - down
			if total > 0
				bflg.prms[2] = oup + total
			  elseif total < 0
				bflg.prms[3] = odw - total
		  else
			bflg.prms[2] = up
			bflg.prms[3] = down

// we need to get all the cases below correct, and more. What about upgrades as well as adding?
		return .ReDest(bflg, dest)
		
		// do this later. its too complex to solve right now.
		// BFLG: r3,  r1, 56, 56, 0
		// BFLG: t31, r3, 32, 31, 1 // add to prev up

		// BFLG: r3,  r1, 56, 56, 0
		// BFLG: t31, r3, 31, 32, 1 // add to prev down

		// BFLG: r3,  r1, 56, 56, 0
		// BFLG: t31, r3, 32, 32, 1 // nop. its less.

		// BFLG: r3,  r1, 32, 32, 0
		// BFLG: t31, r3, 56, 56, 1 // upgrade to 56!

		// BFLG: r3,  r1, 56, 32, 0
		// BFLG: t31, r3, 32, 56, 1 // upgrade to 56, 56
									// the bottom left (32) has to fit in the top right (32)
		// BFLG: r3,  r1, 32, 56, 0
		// BFLG: t31, r3, 56, 32, 1 // can't optimise!



	function BFLG  (|message| exp, |asmreg| dest, |asmreg| Src, |int| up, |int| down, |asmreg|)
		|| SrcBits = src.BitCount
		if (up > srcbits or down > SrcBits)	 and !src.IsBool
			problem (exp, "Shift too far")
		
		if !up and !down
			return src.µtype(Dest)
		
		down += 64 - (SrcBits+up)
		up = 64 - SrcBits
		
		if  src.iszero  or  (up >= 64)  or  (!src.signed and down >= 64)
			return .zeros(exp, dest)
		
		
		|| old = src.fat
		if (old isa asm.bflg)
			|| Merge = .MergeBflg(exp, dest, src, up, down, old)
				return Merge
			
		|| fat = exp.BFLG(dest,  src,  up,  down,  src.signed|int|) 
		return fat.Vectorise(dest, asm.QFLG)


	function Zeros (|message| exp, |asmreg| dest, |asmreg|)
		return .NumToReg(exp, dest, 0, dest.µtype)
				
	
	function ImproveAssign (|asmreg| dest, |asmreg| src, |asmreg|)
		opt inline
		|| f = src.fat$
		
		if src isnt temp
			require .iscurr(f) and (dest is exitatall)
			|| vdecls = .state.parentvars|uint|
			|| find = 1<<src.reg
			require (vdecls & find) // the decl is out of range! IE... it won't be missed.
									// we can fuck around with it as much as we like >:3
		
		return .redest(f, dest)
	
	
	function Assign (|message| exp, |asmreg| dest, |asmreg| src, |asmreg|)  // assigns (
		if src iz dest
			return dest // return src makes more sense? type-wise?
		
		if !src.reg and (src isnt const)
			debugger
			(src is const)
		
		target debug: if (dest.IsVec != src.IsVec) and src.reg
		|| improve = .ImproveAssign(dest, src)
			return improve
		
		if dest.IsVec
			return exp.VMOV(dest, src) * dest

		if src is Const										// setting a const
			|| K = src.const
			return .LoadNumber(exp, K, dest)				// Consts are the most optimisable thing we have

		// BFLG is VERY optimisable :] (but consts are even more)
		
		|| sh = src.BitCount - dest.BitCount
		if sh >= 0
			// leave as is
		  elseif src.signed
			sh = 64 - src.bitcount
		  else
			sh = 0 // no point otherwise. just make the asm simpler.
				
		(dest isnt Const)
		return exp.BFLG(dest, src, sh, sh, src.signed|int|) * Dest
	
	
function int64.Fits (|int| amount, |bool| signed=true,  |bool|)
	|int64| sh = 64 - amount
	|| x = self << sh
	if signed
		x >>= sh
	  else
		x = X|uint64| >> sh
	return x == self

