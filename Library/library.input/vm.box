

// vm.box
forms
	U0:
	U1:		r1 R
	U2:		r1 R,  r2 r
	U3:		r1 R,  r2 r,  r3 r
	U4:		r1 R,  r2 r,  r3 r,  r4 r


	Func	// func: fnc:
		r1		R
		JUMP	l
		Prm1	x
		Prm2	x
// func needs the reg/return wierdness defined also, but as an "ignored" value
// so it comes after the prm info. Like 3 prms takes 15 bits, then 3 more bits for wierdness
// using 32-bits, we can fit 5 regs leaving 7 bits for the total info with 1 bit left to spare.
// this will let us convert our ASM to ARM... without needing extra info.
	ForeignFunc
		r1		R
		Table	l			// functions in registers use the first 32 values...
		Prm1	x
		Prm2	x
	
	Tail
		JUMP	l
		Prm1	x // also
		
	Alloc
		r1		R
		Align	3 // 1 << align
		Amount	l
	
	Jump
		JUMP	j
	
	MinMax		// max:
		r1		R
		r2		r
		r3		r
		Sign	1
		// So... we need to compare
		// but how? We want the biggest item
	
	RotateConst	// RotateConst:
		r1		R
		Rot		6
		Inv		1
		Value	12

	JCmpEq	// cmpeq:
		r1		r
		r2		r
		Jmp		j

	HALT	// cmpeq:
		r1		r
		r2		r
		r3		r
		r4		r
		IsOK	4

	JCmpF	// cmp:
		r1		r
		r2		r
		Cmp		3
		Jmp		j

	JCmpI	// cmp:
		r1		r
		r2		r
		Cmp		3
		Jmp		j

	BraBytes	// BraBytes:
		r1		r
		Bytes	2
		Jmp		j

	Bra		// bra:
		r1		r
		Jmp		j
	
	Loop
		r1		R
		r2		r
		Jmp	j
	
	Table
		r1		R
		Mode	1
		Add		l
	
	Read			// mem:
		r1		R	// dest (big r!)
		r2		r	// Base
		r3		r	// VarAdd
		Offset	7	// Const A
		move	2

	Write		// mem:
		r1		r	// Src (small r!)
		r2		r	// Base
		r3		r	// VarAdd
		Offset	7	// Const A
		move	2

	MemUtil
		r1		R
		r2		r
		Op		3
		N		l

	CNTC
		r1		R
		r2		R
		offset	5
		cnst	7
		size	2 // 1, 2, 4, 8
		// *p1[offset] += cnst

	BFLD
		r1		R
		r2		r
		up		6
		down	6
		sign	1

	CmpB	// cmpb:
		r1		R
		r2		r
		r3		r
		Inv		3

//	BClear
//		r1		R
//		r2		R
//		Shift1	6
//		Shift2	6
//		Sign	2
	
	Convert
		r1		R
		r2		r
		Mode	4
		
	RET					// keep simple for speed
		r1		r		// return reg
		r2		r		// decr
		r3		r		// decr
		Value	6
		Count	3		// amount of returned registers
		
	RETObj				// keep simple for speed
		r1		r		// safedecr
		r2		r		// decr
		r3		r		// decr
		r4		r		// decr

	REQ
		r1		r		// 
		r2		r		// 
		Mode	4		// compare mode for u1 & u2 (==, !=, >, >=, AND, |, & )
						// AND can ! either left or right side
		Val		10		// bitor r2 with Val. 
	
	Div
		r1		R
		r2		R
		r3		r
		r4		r
		Kind	2
	
	Trap
		r1		r
		Mode	2		// 1 = trap on function-count,  2 = trap-always
		Signal	6

	Float
		r1		R
		r2		r
		r3		r
		r4		r
		D		1

	FloatConst
		r1		R
		r2		r
		High	14

	ConstStretchy
		r1		R
		Cond	1	// 0 == always, 1 = only if dest == 0
		Inv		1
		Value	17
		
	AddK
		r1		R
		r2		r
		K		l
	
	Div2
		r1		R
		r2		r
		Sh		6 // could be used to add other numbers...
		Add		6 // in case we don't JUST wanna div by a power of 2
	
	Shift
		r1		R
		r2		r
		r3		r
		Sh		9
	
	Swap
		r1		R
		r2		R
		r3		R
		r4		R

	RefSet
		r1		R
		r2		r
		Incr	1
		Move	1
		Free	1
		Decr	1

	RefSetCode
		r1		R
		r2		r
		Code	4

	RefSet1
		r1		r		// Small R
		r2		r
	
	RefSet2
		r1		r		// Small R
		r2		r
		Decr	1
		Offset	l
	RefSet3
		r1		R		// Big R
		r2		r
		Decr	1
		Offset	l
	RefDecrMem
		r1		r
		Count	7
		Offset	l

	VecMix
		r1		R
		r2		r // can load a float, vec2, or vec3
		r3		r
		r4		r
		Mode	3 // only 6 actual cases. Only list possible entries.


// this file:
// 1) builds the c++ codes, and the jump-table, and the ASM accessor functions
// 2) builds the instruction-info speedie uses exec can just include this file too.
// 3) builds the Âµforms



#!						 (((((STOP)))))
/**/ EROR (HALT)
		if (u2 == u1 and u3 == u4)
			VMFinish

/**/ NOOP (U0)	// does nothing // (could be replaced with bitor 0?)
		i1 = i1
		#!NOOP

/**/ TRAP (Trap)
		JB_App__SelfSignal(Trap_Signalu)



// needs ASM-text: rare, MEMM
#!						 (((((FUNC)))))


/**/ ADDR (U4) 
		u1 = (uint64)(&u2)
		u3 = (uint64)(&u4)

/**/ RET (Ret)				// ret: 
		__ 
		Code = Return1(r, Op)
		___

/**/ RETO (RETObj)			// ret: 
		__ 
		Code = Return2(r, Op)
		___

POS 2
	TAIL (Tail)
		__
		TailStack(r, Code, Op)
		___


POS 24 // force position
	FNC (Func)			// func: 
		__
		Code = BumpStack(vm, r, Code+1, Op, *Code)
		___

POS 25
	FNC3 (Func)
		__
		Code = BumpStack(vm, r, Code+2, Op, Code64)
		___

POS 26
	FNCX (ForeignFunc)
		__
		ForeignFunc(vm, Code, r, Op, *Code)
		Code++ 
		___

POS 27
	FNCX3 (ForeignFunc)
		__
		ForeignFunc(vm, Code, r, Op, Code64) 
		Code += 2
		___


/**/ ALLO (Alloc)
		AllocStack(vm, r, Op)
	


#!						 (((((Utils)))))

/**/ SWAP (Swap)
		std_swap(r[n1], r[n2])
		std_swap(r[n3], r[n4])
/**/ PRNT (U1)
		printf("%lli\n", i1)
		#!better to print this also as float/double/signed/unsigned...
/**/ RARE (U2)
		Rare_(r, Op)
/**/ CONV (Convert)
		RegConv(r, Op)
					// for SPD~>ARM conversion
					// we can just do:	double from double --> int64reg = F64reg 
					//					int64  from int64  --> F64Reg = int64
					// floats or other size ints shouldn't be affected.

#!						 (((((Consts)))))
/**/ KNSR (RotateConst)
		RotateConst(r, Op)

/**/ KNST (ConstStretchy)
		LoadConst(r, Op, 0)

POS 4
	KNST2 (ConstStretchy)
		LoadConst(r, Op, *Code++)   // would be better to do:   (uint64)(*Code++) << 17
									// thats 49 bits at least!
									// LoadConst(r, Op, ConstStretchy_Valueu) --> LoadConst(r, Op, 0)
POS 5
	KNST3 (ConstStretchy)			// this would lose 17 bits? but they are already lost...
									// more like 17 bits become spare and more easy to reuse.
									// perhaps to set simd values!
		LoadConst(r, Op, *((uint64*)(Code)))
		Code+=2


#!						 (((((Math)))))
/**/ ADDK (AddK)
		i1 = i2 + U2_Li
/**/ ADPK (AddK) // post-increment
		i1 = i2
		i2 = i2 + U2_Li
/**/ ADD  (Shift)
		i1 = i2 + (i3 << Shift_Shu)
/**/ SUB  (Shift)              					// must come after ADD
		i1 = (i2 - i3) >> Shift_Shu
/**/ MULT  (U4)
		i1 = (i2 * i3) + i4
/**/ DIVV (Div)
		DivMath(r, Op)
/**/ DIV2 (Div2)
	if (i2 < 0)
		i1 = (i2+((1<<Div2_Addu)-1)) >> Div2_Shu	// quick divide by power of 2
	else
		i1 = i2 >> Div2_Shu
/**/ MAX (MinMax)
		if (MinMax_Signu)
			i1 = std_max(i2, i3)
		else
			u1 = std_max(u2, u3)
/**/ MIN (MinMax)
		if (MinMax_Signu)
			i1 = std_min(i2, i3)
		else
			u1 = std_min(u2, u3)



#!						 (((((Bitops)))))
/**/ BXOR  (Shift)
		u1 = u2 ^     (u3|||Shift_Shu)
/**/ BSHS  (Shift)
		i1 = i2 >>    (u3+Shift_Shu)
/**/ BSHR  (Shift)
		u1 = u2 >>    (u3+Shift_Shu)
/**/ BSHL  (Shift) 
		u1 = u2 <<    (u3+Shift_Shu)
/**/ BAND  (Shift)
		u1 = u2 &     (u3|||Shift_Shu)
/**/ BOR   (Shift)
		u1  = u2 |||  (u3|||Shift_Shu)
/**/ BNAN  (Shift)
		u1 = u2 &    ~(u3|||Shift_Shu)
/**/ BNOR  (Shift)
		u1  = u2 ||| ~(u3|||Shift_Shu)
/**/ BNOT  (Shift)
		u1 = ~u2  &  ~(u3|||Shift_Shu)
/**/ BFLG  (BFLD) // perhaps sign can allow for SIMD opts?
		if (BFLD_signu)
			i1 = ((i2 << BFLD_upu) >> BFLD_downu)
		else
			u1 = ((u2 << BFLD_upu) >> BFLD_downu)	
/**/ BSTT  (U4)
		i1 = 0 // bitstats(u2, u3, u4)
///**/ BCLR  (BClear)
//		BitClear(r, Op)

///**/ BFLS  (BFLD) // seems useless?
//		BFLS(r, Op)
///**/ BROL  (U3) // just make these functions or rare instructions?
//		i1 = JB_u64_RotL(u2, u3 + L3)
///**/ BROR  (U3)
//		i1 = JB_u64_RotR(u2, u3 + L3)



#!						 (((((MiniCompare)))))
/**/ CMPB  (CmpB)
		u1 = BitComp(r, Op)

/**/ TERN  (U4) // could give this a compare mode, like i2 >= 0, or whatever
	if (u2)
		r[n1] = r[n3]
	else
		r[n1] = r[n4]

/**/ CMPI  (JCmpI)
	CompI(r, Op)

/**/ CMPF  (JCmpF)
	CompF(r, Op)



#!						 (((((Branches)))))
/**/ JSWI (jcmpeq)
		Code += n1+n2+JCmpEq_Jmpi

/**/ JUMP (jump)
		Code += l0

/**/ JMPI  (JCmpI)
		__
		Code = JumpI(r, Op, Code)
		___
/**/ JMPF  (JCmpF)
		__
		Code = JumpF(r, Op, Code)
		___
/**/ JMPE  (JCmpEq)
		__
		Code = JumpEq(r, Op, Code)
		___
/**/ JMPN  (JCmpEq)
		__
		Code = JumpNeq(r, Op, Code)
		___
/**/ JBOR  (Bra)
		__
		if (i1)
			Code += Bra_Jmpi
		___
/**/ JBAN  (Bra)					
		__
		if (!i1)
			Code += Bra_Jmpi
		___
/**/ LUPD  (Loop)			 	// same
		__
		if (i1-- > i2)
			Code += Loop_Jmpi
		___
/**/ LUPU  (Loop)				// assume uint64
		__
		if (i1++ < i2)
			Code += Loop_Jmpi
		___



#!						 (((((Refs)))))
/**/ RFAP  (U3)
		SetRefApart(r, Op)

/**/ RFST  (RefSet1)
		SetRefBasic(r, Op)		

/**/ RFWR  (RefSet2)							// RefCount reg --> mem
		SetRefRegToMem(r, Op)

/**/ RFRD  (RefSet3)							// RefCount reg <-- mem
		SetRefMemToReg(r, Op)

/**/ RFDC  (RefDecrMem)							// for destructors/closers.
		SetRefDecrMem(r, Op)					// unused rn ðŸ˜­ *sadface*

/**/ RALO  (U2)
		o1 = alloc(o2)
		#!should call constructor too.



#!						(((((Memory)))))
/**/ GSTR  (table)
		o1 = strs(vm, Op)
/**/ GTAB  (table) // we could make a GLRD and GLWR instruction. faster global read/writes
		u1 = table(vm, Op)
/**/ RD1U  (Read)
		u1 = mem1(uint8)
		mem2(uint8)
/**/ RD1S  (Read)
		u1 = mem1(char)
		mem2(char)
/**/ RD2U  (Read)
		u1 = mem1(u16)
		mem2(u16)
/**/ RD2S  (Read)
		u1 = mem1(s16)
		mem2(s16)
/**/ RD4U  (Read)
		u1 = mem1(u32)
		mem2(u32)
/**/ RD4S  (Read)
		u1 = mem0(int)
		mem2(int)
/**/ RD8U  (Read)
		u1 = mem1(u64)
		mem2(u64)
/**/ RD16  (Read)
		((ivec4*)r)[n1] = mem1(ivec4)
		mem2(ivec4)
/**/ WR1U  (Write)
		mem1(uint8) = u1
		mem2(uint8)
/**/ WR2U  (Write)
		mem1(u16) = u1
		mem2(u16)
/**/ WR4U  (Write)
		#! xcode only complains about this one? ?
		mem1(u32) = (u32)u1
		mem2(u32)
/**/ WR8U  (Write)
		mem1(u64) = u1
		mem2(u64)
/**/ WR16  (Write)
		mem1(ivec4) = (ivec4)(((ivec4*)r)[n1])
		mem2(ivec4)

/**/ CNTC  (CNTC) // negation bound CNTC and CNTD. CNTD must always be following.
		IncrementAddr(r, Op, 1) // same with a few others actually

/**/ CNTD  (CNTC)
		IncrementAddr(r, Op, 0)

/**/ MEMU  (MemUtil)
		#! copy/fill/endian/xor
		MemStuff((u32*)u1, (u32*)u2, n3, L3)


#!						(((((Float)))))

/*
/**/ FEXK  (FloatAddExp)
		if FloatAddExp_Du
			f1 = FloatSh2(u2, FloatAddExp_Sh2i) + FloatSh2(u3, FloatAddExp_Sh3i)
		else
			f1 = FloatSh1(u2, FloatAddExp_Sh2i) + FloatSh1(u3, FloatAddExp_Sh3i)
*/		

/**/ FADD  (Float)
		if (Float_Du)
			d1 = d2 + d3 - d4
		else
			f1 = f2 + f3 - f4

/**/ FADK (FloatConst)
		// we just want simple things like MyFloat += 2.0
		f1 += f2 + FloatIncr1(Op)

/**/ FMUL  (Float)
		if (Float_Du)
			d1 = (d2 * d3)+d4
		else
			f1 = (f2 * f3)+f4

/**/ FMLK  (FloatConst)
		f1 = f2 * FloatIncr1(Op)

/**/ FDIV  (float)
		if (Float_Du)
			d1 = d2 / d3
		else
			f1 = f2 / f3

/**/ FFRC  (Float)
		if (Float_Du)
			d1 = (d2 - floor(d2)) * d3
		else
			f1 = (f2 - floor(f2)) * f3

/**/ FMAX  (Float)
		if (Float_Du)
			d1 = std_max(d2, d3)
		else
			f1 = std_max(f2, f3)

/**/ FMIN  (Float)
		if (Float_Du)
			d1 = std_min(d2, d3)
		else
			f1 = std_min(f2, f3)
