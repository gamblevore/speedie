

// vm.box
forms
	U0:
	U1:		r1 R
	U2:		r1 R,  r2 r
	U3:		r1 R,  r2 r,  r3 r
	U4:		r1 R,  r2 r,  r3 r,  r4 r


	Func	// func: fnc:
		r1		R
		JUMP	l
		Prm1	x
		Prm2	x

// func needs the reg/return wierdness defined also, but as an "ignored" value
// so it comes after the prm info. Like 3 prms takes 15 bits, then 3 more bits for wierdness
// using 32-bits, we can fit 5 regs leaving 7 bits for the total info with 1 bit left to spare.
	ForeignFunc
		r1		R
		Table	l			// functions in registers use the first 32 values...
		Prm1	x
		Prm2	x
	
	Tail
		JUMP	l
		Prm1	x
		
	Alloc
		r1		R
		Amount	l
	
	Jump
		JUMP	j
		
	RotateConst	// RotateConst:
		r1		R
		Rot		6
		Inv		1
		Value	12

	JCmpEq	// cmpeq:
		r1		r
		r2		r
		Big		1
		Jmp		j

	HALT
		r1		r
		r2		r
		r3		r
		r4		r
		IsOK	4

	AddOrSubM
		r1		R
		r2		r
		r3		r
		Sh		6

	JCmpF
		r1		r
		r2		r
		Cmp		3
		Jmp		j

	JCmpI
		r1		r
		r2		r
		Cmp		4
		Jmp		j

	JCmpK	 // still needs the big/small thing. do later.
		r1		r
		Big		1
		K		9
		Jmp		j

	Bra		// bra:
		r1		r
		Big		1
		Jmp		j
	
	Loop
		r1		R
		r2		r
		Big		1
		Jmp		j
	
	Table
		r1		R
		Mode	1
		Add		l
	
	Read			// mem:
		r1		R	// dest (big r!)
		r2		r	// Base
		r3		r	// VarAdd
		Offset	7	// Const A
		move	2

	Write		// mem:
		r1		r	// Src (small r!)
		r2		r	// Base
		r3		r	// VarAdd
		Offset	7	// Const A
		move	2

	MemUtil
		r1		R
		r2		r
		Op		3
		N		l
	

	CNTC
		r1		R
		r2		R
		offset	5
		cnst	7
		size	2 // 1, 2, 4, 8
		// *p1[offset] += cnst

	BFLD
		r1		R
		r2		r
		up		6
		down	6
		sign	1

	CmpB	// cmpb:
		r1		R
		r2		r
		r3		r
		Inv		3
	
	Convert
		r1		R
		r2		r
		Mode	4
		
	RET
		r1		r		// return reg1
		r2		r		// decr
		r3		r		// decr
		SafeDecr 1
		Value	8

	REQ
		r1		r		// 
		r2		r		// 
		Mode	4		// compare mode for u1 & u2 (==, !=, >, >=, AND, |, & )
						// AND can ! either left or right side
		Val		10		// bitor r2 with Val. 
	
	Div
		r1		R
		r2		R
		r3		r
		r4		r
		Kind	2
	
	Trap
		r1		r
		Mode	2		// 1 = trap on function-count,  2 = trap-always
		Signal	6

	Float
		r1		R
		r2		r
		r3		r
		r4		r
		D		1

	FloatConst
		r1		R
		r2		r
		High	14

	ConstStretchy
		r1		R
		Cond	1	// 0 == always, 1 = only if dest == 0
		Inv		1
		Value	17
		
	ConstGlobal
		r1		R
		Value	17
		
	AddK
		r1		R
		r2		r
		K		l
	
	Div2
		r1		R
		r2		r
		Sh		6   // 6 bits left unused? Oh well?
		Clear	3
	
	Shift
		r1		R
		r2		r
		r3		r
		Sh		9
	
	Swap
		r1		R
		r2		R
		r3		R
		r4		R

	RefSetCode
		r1		R
		r2		r
		Code	4

	RefSet1
		r1		r		// Small R
		r2		r

	RefSetApart
		r1		r
		r2		r
		r3		r
	
	RefSet2
		r1		r		// Small R
		r2		r
		Decr	1
		Offset	l
	
	RefSet3
		r1		R		// Big R
		r2		r
		Decr	1
		Offset	l
	
	RefDecrMem
		r1		r
		Count	7
		Offset	l

	VecMix
		r1		R
		r2		r // can load a float, vec2, or vec3
		r3		r
		r4		r
		Mode	3 // only 6 actual cases. Only list possible entries.


	VecGet
		r1		R
		r2		r
		r3		r
		Ind		2
	
	
	VecBuild
		r1		R
		r2		r
		r3		r
		r4		r
		r5		4

	VecSwizzle
		r1		R
		r2		r
		Fields	12
		
	VecConst
		r1		R
		K1		l
		K2		x
		
	
// this file:
// 1) builds the c++ codes, and the jump-table, and the ASM accessor functions
// 2) builds the instruction-info speedie uses exec can just include this file too.
// 3) builds the Âµforms



#!						 (((((STOP)))))
/**/ EROR (HALT)
		if (u2 == u1 and u3 == u4)
			VMFinish

/**/ NOOP (U0)	// does nothing // (could be replaced with bitor 0?)
		i1 = i1
		#!NOOP

/**/ TRAP (Trap)
		JB_App__SelfSignal(Trap_Signalu)



// needs ASM-text: rare, MEMM
#!						 (((((FUNC)))))
/**/ GRAB (U2) 
		u1 = (uint64)(&u2)
//		u3 = (uint64)(&u4)

/**/ RET (Ret)				// ret: 
		Code = ReturnFromFunc(vm, r, Op)

POS 2
	TAIL (Tail)
		TailStack(vm, r, Code, Op)


POS 24 // force position
	FNC (Func)			// func: 
		Code = BumpStack(vm, r, Code+1, Op, *Code)

POS 25
	FNC3 (Func)
		Code = BumpStack(vm, r, Code+2, Op, Code64)

POS 26
	FNCX (ForeignFunc)
		ForeignFunc(vm, Code, r, Op, *Code)
		Code++ 

POS 27
	FNCX3 (ForeignFunc)
		ForeignFunc(vm, Code, r, Op, Code64) 
		Code += 2


/**/ ALLO (Alloc)
		AllocStack(vm, r, Op)



#!						 (((((Utils)))))

/**/ SWAP (Swap)
		JB_Swap(r[n1], r[n2])
		JB_Swap(r[n3], r[n4])
///**/ PRNT (U1)
//		printf("%lli\n", i1)
//		#!better to print this also as float/double/signed/unsigned...
/**/ RARE (U2)
		Rare_(r, Op)
/**/ CONV (Convert)
		RegConv(r, Op)
					// for SPD~>ARM conversion
					// we can just do:	double from double --> int64reg = F64reg 
					//					int64  from int64  --> F64Reg = int64
					// floats or other size ints shouldn't be affected.



#!						 (((((Consts)))))
/**/ KNSR (RotateConst)
		RotateConst(r, Op)

/**/ KNST (ConstStretchy)
		LoadConst(r, Op, 0)

POS 4
	KNST2 (ConstStretchy)
		LoadConst(r, Op, *Code++)   // would be better to do:   ((uint64)(*Code++) << 17) >> 17
									// thats 49 bits at least!
									// LoadConst(r, Op, ConstStretchy_Valueu) --> LoadConst(r, Op, 0)
POS 5
	KNST3 (ConstStretchy)			// this would lose 17 bits? but they are already lost...
									// more like 17 bits become spare and more easy to reuse.
									// perhaps to set simd values!
		LoadConst(r, Op, *((uint64*)(Code)))
		Code+=2

//POS 6
//	GMEM (ConstGlobal) // for global memory adresses
//		LoadGConst(r, Op, *Code++)



#!						 (((((Math)))))

/**/ ADDK (AddK)
		i1 = i2 + U2_Li
/**/ ADPK (AddK) // post-increment
		i1 = i2
		i2 = i2 + U2_Li
/**/ MULK  (AddK)
		i1 = i2 * U2_Li
/**/ ADD  (Shift)
		i1 = i2 + (i3 << Shift_Shu)
/**/ SUB  (Shift)              						// must come after ADD
		i1 = (i2 - i3) >> Shift_Shu
/**/ MUL  (U4)
		i1 = (i2 * i3) + i4
/**/ DIV (Div)
		DivMath(r, Op)
/**/ DIV2 (Div2)
		if (i2 < 0)	// quick divide by power of 2 on a signed int
			i1 = (((i2+((1<<Div2_Shu)-1)) >> Div2_Shu) << Div2_Clearu) >> Div2_Clearu
		else
			i1 = ((i2 >> Div2_Shu) << Div2_Clearu) >> Div2_Clearu

/**/ CLAMI (U4)
		i1 = std_min(std_max(i2, i3), i4)
/*			min(y,H) --> clamp(y, y, H)
			max(y,L) --> clamp(y, L, L)			*/
/**/ CLAMU (U4)
		u1 = std_min(std_max(u2, u3), u4)



#!					((((((((32-bit math))))))))
// Need signed/unsigned versions...        0xFFFFffff != -1|int64|
/**/ ADDM (AddOrSubM) // mixed math
		i1 = i2 + (ii3 << AddOrSubM_Shu)
/**/ SUBM (AddOrSubM)
		i1 = (i2 - ii3) >> AddOrSubM_Shu
/**/ DIVS (Div)
		DivMath32(r, Op)
/**/ MULS  (U4)
		i1 = (ii2 * ii3) + ii4
///**/ ADKS (AddK)
//		i1 = ii2 + U2_Li
///**/ APKS (AddK)										// post-increment
//		i1 = ii2
//		i2 = ii2 + U2_Li
///**/ MLKS  (AddK)
//		i1 = ii2 * U2_Li
///**/ ADDS  (Shift)
//		i1 = ii2 + (ii3 << Shift_Shu)
///**/ SUBS  (Shift)              						// must come after ADD
//		i1 = (ii2 - ii3) >> Shift_Shu




#!						 (((((Bitshifts)))))
/**/ BFLG  (BFLD) // perhaps sign can allow for SIMD opts?
		if (BFLD_signu)
			i1 = ((i2 << BFLD_upu) >> BFLD_downu)
		else
			u1 = ((u2 << BFLD_upu) >> BFLD_downu)	
/**/ BRSS  (Shift)
		i1 = ((i2 << Shift_Shu) >> Shift_Shu) >> i3
/**/ BRSH  (Shift)
		u1 = ((u2 << Shift_Shu) >> Shift_Shu) >> u3
/**/ BLSH  (Shift) 
		u1 = ((u2 << u3) << Shift_Shu) >> Shift_Shu



#!						 (((((BitOps)))))
/**/ BSTT  (U4)
		i1 = 0 // bitstats(u2, u3, u4)

/**/ BAND  (Shift)
		u1 = u2  &  ((u3 << Shift_Shu) >> Shift_Shu)
/**/ BXOR  (Shift)
		u1 = u2  ^  ((u3 << Shift_Shu) >> Shift_Shu)
/**/ BORR  (Shift)
		u1 = u2 ||| ((u3 << Shift_Shu) >> Shift_Shu)
/**/ BNOT  (Shift)
		u1 = ~u2 &  ((u3 << Shift_Shu) >> Shift_Shu)
/**/ BANDK  (AddK)
		u1 = u2  &  U2_Li
/**/ BXORK  (AddK)
		u1 = u2  ^  U2_Li
/**/ BORRK   (AddK)
		u1 = u2 ||| U2_Li
/**/ BNOTK  (AddK)
		u1 = ~u2 & ~U2_Li



#!						 (((((MiniCompare)))))
/**/ CMPB  (CmpB) // do we need this now? If everything is 32-bit correct at least?
		u1 = BitComp(r, Op)

/**/ TERN  (U4)
		if (u2) // needs a shift
			r[n1] = r[n3]
		else
			r[n1] = r[n4]

/**/ CMPI  (JCmpI)
		CompI(r, Op)

/**/ CMPF  (JCmpF)
		CompF(r, Op)



#!						 (((((Branches)))))
///**/ JSWI (JCmpEq)
//		Code += n1+n2+JCmpEq_Jmpi

/**/ JUMP (jump)
		Code += l0

/**/ JMPI  (JCmpI)
		Code = JumpI(r, Op, Code)

/**/ JMPF  (JCmpF)
		Code = JumpF(r, Op, Code)

/**/ JMPE  (JCmpEq)
		Code = JumpEq(r, Op, Code)

/**/ JMPN  (JCmpEq)
		Code = JumpNeq(r, Op, Code)

/**/ JMPK  (JCmpK)
		Code = JumpK(r, Op, Code)

/**/ JMPKN  (JCmpK)
		Code = JumpKN(r, Op, Code)
	
/**/ JBOR  (Bra)
		if (u1)
			Code += Bra_Jmpi
/**/ JBAN  (Bra)				
		if (!u1)
			Code += Bra_Jmpi
/**/ LUPU  (Loop)				// need LUPUS
		if (i1++ < i2)
			Code += Loop_Jmpi
/**/ LUPD  (Loop)			 	// need LUPDS
		if (i1-- > i2)
			Code += Loop_Jmpi



#!						 (((((Refs)))))
/**/ RFAP  (RefSetApart)
		SetRefApart(r, Op)

/**/ RFST  (RefSet1)
		SetRefBasic(r, Op)		

/**/ RFWR  (RefSet2)							// RefCount reg --> mem
		SetRefRegToMem(r, Op)

/**/ RFRD  (RefSet3)							// RefCount reg <-- mem
		SetRefMemToReg(r, Op)

/**/ RFDC  (RefDecrMem)							// for destructors/closers.
		SetRefDecrMem(r, Op)					// unused rn ðŸ˜­ *sadface*

/**/ RALO  (U2)
		o1 = alloc(o2)
		#!should call constructor too.



#!						(((((Memory)))))
/**/ GSTR  (table)
		o1 = strs(vm, Op)
/**/ GTAB  (table) // we could make a GLRD and GLWR instruction. faster global read/writes
		u1 = table(vm, Op)
/**/ RD1U  (Read)
		u1 = mem1(uint8)
		mem2(uint8)
/**/ RD1S  (Read)
		u1 = mem1(char)
		mem2(char)
/**/ RD2U  (Read)
		u1 = mem1(u16)
		mem2(u16)
/**/ RD2S  (Read)
		u1 = mem1(s16)
		mem2(s16)
/**/ RD4U  (Read)
		u1 = mem1(u32)
		mem2(u32)
/**/ RD4S  (Read)
		u1 = mem0(int)
		mem2(int)
/**/ RD8U  (Read)
		u1 = mem1(u64)
		mem2(u64)
/**/ RD16  (Read)
		((ivec4*)r)[n1] = mem1(ivec4)
		mem2(ivec4)
/**/ WR1U  (Write)
		mem1(uint8) = u1
		mem2(uint8)
/**/ WR2U  (Write)
		mem1(u16) = u1
		mem2(u16)
/**/ WR4U  (Write)
		#! xcode only complains about this one? ?
		mem1(u32) = (u32)u1
		mem2(u32)
/**/ WR8U  (Write)
		mem1(u64) = u1
		mem2(u64)
/**/ WR16  (Write)
		mem1(ivec4) = (ivec4)(((ivec4*)r)[n1])
		mem2(ivec4)

/**/ CNTC  (CNTC) // negation bound CNTC and CNTD. CNTD must always be following.
		IncrementAddr(r, Op, 1) // same with a few others actually

/**/ CNTD  (CNTC)
		IncrementAddr(r, Op, 0)

/**/ MEMZ  (ConstGlobal)	// zero
		memzero((void*)u1, L1)



#!						(((((Float)))))

/**/ FADD  (Float)
		if (Float_Du)
			d1 = d2 + d3 - d4
		else
			f1 = f2 + f3 - f4

/**/ FADK (FloatConst)
		// we just want simple things like MyFloat += 2.0
		f1 += f2 + FloatIncr1(Op)

/**/ FMUL  (Float)
		if (Float_Du)
			d1 = (d2 * d3)+d4
		else
			f1 = (f2 * f3)+f4

/**/ FMLK  (FloatConst)
		f1 = f2 * FloatIncr1(Op)

/**/ FDIV  (float)
		if (Float_Du)
			d1 = d2 / d3
		else
			f1 = f2 / f3

/**/ FFRC  (Float)
		if (Float_Du)
			d1 = (d2 - floor(d2)) * d3
		else
			f1 = (f2 - floor(f2)) * f3

/**/ FCLM (Float)
		if (Float_Du)
			d1 = std_min(std_max(d2,d3),d4)
		else
			f1 = std_min(std_max(f2,f3),f4)



#!						(((((Vectors)))))

/**/ VGET (VecGet)
		i1 = iv2[n3+u4]
/**/ VSET (VecGet)
		iv1[n3+u4] = i2
/**/ VBLD (VecBuild)
		v1 = VBuild(r, Op)
/**/ VSWZ (VecSwizzle)
		v1 = VSwiz(r, Op)
/**/ VMOV (U4)
		iv1 = iv2
		iv3 = iv4



#!						(((((Float VecMath)))))
/**/ VADD  (Float)
		v1 = v2 + v3 - v4
/**/ VADK (FloatConst)
		v1 += v2 + FloatIncr1(Op)
/**/ VMUL  (Float)
		v1 = (v2 * v3)+v4
/**/ VMLK  (FloatConst)
		v1 = v2 * FloatIncr1(Op)
/**/ VDIV  (float)
		v1 = v2 / v3
/**/ VFRC  (Float)
		v1 = (v2 - VFloor(v2)) * v3
/**/ VCLM (Float)
		v1 = VMin(VMax(v2,v3),v4)



#!						(((((Integer VecMath)))))
/**/ QADD  (Shift)
		iv1 = iv2 + (iv3 << Shift_Shu)
/**/ QSUB  (Shift)              						// must come after ADD
		iv1 = (iv2 - iv3) >> Shift_Shu
/**/ QMUL  (U4)
		iv1 = (iv2 * iv3) + iv4
/**/ QDIV (Div)
		iv1 = iv2 / iv3
/**/ QCLM (U4)
		iv1 = QMin(QMax(iv2,iv3),iv4)
/**/ QADK (AddK)
		iv1 = iv2 + U2_Li



#!						 (((((Vector Bitshifts)))))
/**/ QFLG  (BFLD)
		iv1 = ((iv2 << BFLD_upu) >> BFLD_downu)
/**/ QRSS  (Shift)
		iv1 = (iv2 >> Shift_Shu) >> iv3
/**/ QRSH  (Shift)
		uv1 = (uv2 >> Shift_Shu) >> uv3
/**/ QLSH  (Shift) 
		uv1 = (uv2 << uv3) << Shift_Shu



#!						 (((((Vector BitOps)))))
/**/ QAND  (Shift)
		uv1 = uv2  &  (uv3 ||| Shift_Shu)
/**/ QXOR  (Shift)
		uv1 = uv2  ^  (uv3 ||| Shift_Shu)
/**/ QORR  (Shift)
		uv1 = uv2 ||| (uv3 ||| Shift_Shu)
/**/ QNOT  (Shift)
		uv1 = ~uv2 & ~(uv3 ||| Shift_Shu)

